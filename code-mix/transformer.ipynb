{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.k_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.v_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key : torch.Tensor, value : torch.Tensor, type, mask=None, mask2=None):\n",
    "        # query shape: [batch_size, query_len, embedding_dim]\n",
    "        # key shape: [batch_size, key_len, embedding_dim]\n",
    "        # value shape: [batch_size, value_len, embedding_dim]\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Linear transformations\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # print(\"hiii\")\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            if type == 'encoder':\n",
    "                mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
    "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "            elif type == 'decoder':\n",
    "                mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
    "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "                mask2 = mask2.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
    "                scores = scores.masked_fill(mask2 == 0, float('-inf'))\n",
    "            elif type == 'encoder-decoder':\n",
    "                final_mask = mask2.transpose(-2, -1) @ mask\n",
    "                final_mask = final_mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
    "                scores = scores.masked_fill(final_mask == 0, float('-inf'))\n",
    "\n",
    "\n",
    "        attn_weights = nn.functional.softmax(scores, dim=-1)\n",
    "        attn_weights = attn_weights.masked_fill(torch.isnan(attn_weights), 0)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # print(attn_weights.shape)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Reshape\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.embedding_dim)\n",
    "        # print(attn_output.shape)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PositionwiseFeedforward(nn.Module):\n",
    "    def __init__(self, embedding_dim, dff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, dff)\n",
    "        self.fc2 = nn.Linear(dff, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, context_size, embedding_dim]\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(embedding_dim, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedforward(embedding_dim, 4*embedding_dim)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src shape: [batch_size, src_len, hidden_dim]\n",
    "        # src_mask shape: [batch_size, 1, 1, src_len]\n",
    "        # print(\"Hello\")\n",
    "        # print(src.shape)\n",
    "        src2 = self.norm1(src)\n",
    "        # print(\"Hello2\")\n",
    "        src = src + self.dropout(self.self_attn(src2, src2, src2, 'encoder', src_mask))\n",
    "        src2 = self.norm2(src)\n",
    "        src = src + self.dropout(self.feed_forward(src2))\n",
    "        # print(\"Hello3\")\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, context_size, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(context_size, embedding_dim)\n",
    "        position = torch.arange(0, context_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, embedding_dim]\n",
    "        x = x + self.pe[:x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_size = context_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embedding_dim, context_size, dropout)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(embedding_dim, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, src, src_lens):\n",
    "        # src shape: [batch_size, src_len]\n",
    "        src_mask = self.calculate_mask(src, src_lens)\n",
    "        # src_mask shape: [batch_size, 1, 1, src_len]\n",
    "        src = self.embedding(src)\n",
    "        # src shape: [batch_size, src_len, context_size]\n",
    "\n",
    "        src = self.pos_encoding(src)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        src = self.norm(src)\n",
    "        return src, src_mask\n",
    "\n",
    "    def calculate_mask(self, src, src_lens):\n",
    "        # src shape: [batch_size, context_size]\n",
    "        # src_lens shape: [batch_size]\n",
    "        # mask shape: [batch_size, hidden_dim, hidden_dim]\n",
    "        batch_size = src.shape[0]\n",
    "        context_size = src.shape[1]\n",
    "        mask = torch.arange(context_size).expand(batch_size, context_size).to(device) < src_lens.clone().detach().unsqueeze(1).to(device)\n",
    "        mask = mask.float()\n",
    "        mask = mask.unsqueeze(1)\n",
    "        mask = mask.transpose(1, 2) @ mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.self_attn = MultiHeadSelfAttention(embedding_dim, num_heads)\n",
    "        self.enc_attn = MultiHeadSelfAttention(embedding_dim, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedforward(embedding_dim, 4*embedding_dim)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm3 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # trg shape: [batch_size, context_size, embedding_dim]\n",
    "        # enc_src shape: [batch_size, context_size, embedding_dim]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_triangle = torch.tril(torch.ones((trg.shape[0], trg_len, trg_len), device=trg.device)).bool().float()\n",
    "        trg2 = self.norm1(trg)\n",
    "\n",
    "        trg = trg + self.dropout(self.self_attn(trg2, trg2, trg2, 'decoder', trg_mask, trg_triangle))\n",
    "\n",
    "        trg2 = self.norm2(trg)\n",
    "        trg = trg + self.dropout(self.enc_attn(trg2, enc_src, enc_src, 'encoder-decoder', src_mask, trg_mask))\n",
    "\n",
    "        trg2 = self.norm3(trg)\n",
    "        trg = trg + self.dropout(self.feed_forward(trg2))\n",
    "        return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embedding_dim, context_size, dropout)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(embedding_dim, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, trg, trg_lens, enc_src, src_mask, inference=False):\n",
    "        # trg shape: [batch_size, context_size]\n",
    "        # enc_src shape: [batch_size, context_size, embedding_dim]\n",
    "        if inference == False:\n",
    "            trg_mask = self.calculate_mask(trg, trg_lens)\n",
    "            # trg_mask shape: [batch_size, context_size, context_size]\n",
    "            trg = self.embedding(trg)\n",
    "            # trg shape: [batch_size, context_size, embedding_dim]\n",
    "            trg = self.pos_encoding(trg)\n",
    "\n",
    "            for layer in self.layers:\n",
    "                trg = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "            trg = self.norm(trg)\n",
    "            output = self.fc_out(trg)\n",
    "            return output\n",
    "        else:\n",
    "            return self.inference(enc_src, src_mask, trg.shape[1], torch.tensor([32883]), torch.tensor([[32884]], device=trg.device))\n",
    "\n",
    "    # def inference(self, enc_src, src_mask, max_len, start_token, end_token):\n",
    "    #     # enc_src shape: [batch_size, src_len, hidden_dim]\n",
    "    #     # src_mask shape: [batch_size, 1, 1, src_len]\n",
    "    #     # max_len: maximum length of the generated sequence\n",
    "    #     # start_token: integer representing the start token\n",
    "    #     # end_token: integer representing the end token\n",
    "    #     # print(start_token)\n",
    "    #     batch_size = enc_src.shape[0]\n",
    "    #     trg = torch.tensor([[start_token]] * batch_size, device=enc_src.device)\n",
    "    #     # trg shape: [batch_size, 1]\n",
    "    #     output = []\n",
    "    #     for i in range(max_len):\n",
    "    #         trg_mask = self.calculate_mask(trg)\n",
    "    #         # trg_mask shape: [batch_size, 1, i+1, i+1]\n",
    "    #         trg_embedded = self.embedding(trg) * math.sqrt(self.hidden_dim)\n",
    "    #         # trg_embedded shape: [batch_size, i+1, hidden_dim]\n",
    "    #         trg_embedded = self.pos_encoding(trg_embedded)\n",
    "    #         # print(trg_embedded.shape)\n",
    "    #         for layer in self.layers:\n",
    "    #             trg_embedded = layer(trg_embedded, enc_src, trg_mask, src_mask)\n",
    "    #         trg_embedded = self.norm(trg_embedded)\n",
    "    #         # print(trg_embedded.shape)\n",
    "    #         logits = self.fc_out(trg_embedded[:, -1])\n",
    "\n",
    "    #         # logits shape: [batch_size, vocab_size]\n",
    "    #         probs = F.softmax(logits, dim=-1)\n",
    "    #         # print(probs.shape)\n",
    "    #         # probs shape: [batch_size, vocab_size]\n",
    "    #         next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "    #         # next_token shape: [batch_size, 1]\n",
    "    #         output.append(logits.unsqueeze(1))\n",
    "    #         trg = torch.cat([trg, next_token], dim=-1)\n",
    "    #         # trg shape: [batch_size, i+2]\n",
    "    #         if torch.all(next_token == end_token):\n",
    "    #             break\n",
    "    #     output = torch.cat(output, dim=1)\n",
    "    #     # print(output.shape)\n",
    "    #     # output shape: [batch_size, seq_len]\n",
    "    #     return output\n",
    "\n",
    "    def calculate_mask(self, trg, trg_lens):\n",
    "        # trg shape: [batch_size, context_size]\n",
    "        # trg_lens shape: [batch_size]\n",
    "        # mask shape: [batch_size, context_size, context_size]\n",
    "        batch_size = trg.shape[0]\n",
    "        context_size = trg.shape[1]\n",
    "        mask = torch.arange(context_size).expand(batch_size, context_size).to(device) < trg_lens.clone().detach().unsqueeze(1).to(device)\n",
    "        mask = mask.float()\n",
    "        mask = mask.unsqueeze(1)\n",
    "        mask = mask.transpose(1, 2) @ mask\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self, encoder_vocab_size, embedding_dim, context_size, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(encoder_vocab_size, embedding_dim, context_size, num_layers, num_heads, dropout)\n",
    "        self.fc = nn.Linear(context_size, 1)\n",
    "\n",
    "    def forward(self, src, src_lens, inference=False):\n",
    "        # src shape: [batch_size, context_size]\n",
    "        # trg shape: [batch_size, context_size]\n",
    "        # src_lens shape: [batch_size]\n",
    "        # trg_lens shape: [batch_size]\n",
    "\n",
    "        enc_src, src_mask = self.encoder(src, src_lens)\n",
    "        output = self.fc(enc_src[:,0,:])\n",
    "\n",
    "        # enc_src shape: [batch_size, context_size, embedding_dim]\n",
    "        # src_mask shape: [batch_size, context_size, context_size]\n",
    "\n",
    "        # output = self.decoder(trg, trg_lens, enc_src, src_mask, inference)\n",
    "        # output shape: [batch_size, context_size, decoder_vocab_size]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load CSV data\n",
    "train_df = pd.read_csv('codemix-main/train.csv')\n",
    "test_df = pd.read_csv('codemix-main/test.csv')\n",
    "validation_df = pd.read_csv('codemix-main/valid.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>labels</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ather farouqui general secretary of ghar empha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>by passing of is started ji jaggo nahi to sama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>swadu duniya geeta parjapat manjeetgill royal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>hurry up kahin ye offer miss na ho jaye p p p p p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>s logic hasne ke paise milte hai to alag alag ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114995</th>\n",
       "      <td>114995</td>\n",
       "      <td>0</td>\n",
       "      <td>jab pakistan me flods aaty hain tw or pani cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114996</th>\n",
       "      <td>114996</td>\n",
       "      <td>0</td>\n",
       "      <td>vineeta chadha wo jo ke time sab ne waste kiya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114997</th>\n",
       "      <td>114997</td>\n",
       "      <td>0</td>\n",
       "      <td>mujhe bhi do na caption credits akhil thakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114998</th>\n",
       "      <td>114998</td>\n",
       "      <td>0</td>\n",
       "      <td>people of balochistangilgit and pok have thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114999</th>\n",
       "      <td>114999</td>\n",
       "      <td>1</td>\n",
       "      <td>jo it really should instead of doing this essa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  labels                                             tweets\n",
       "0                0       0  ather farouqui general secretary of ghar empha...\n",
       "1                1       0  by passing of is started ji jaggo nahi to sama...\n",
       "2                2       1  swadu duniya geeta parjapat manjeetgill royal ...\n",
       "3                3       1  hurry up kahin ye offer miss na ho jaye p p p p p\n",
       "4                4       1  s logic hasne ke paise milte hai to alag alag ...\n",
       "...            ...     ...                                                ...\n",
       "114995      114995       0  jab pakistan me flods aaty hain tw or pani cho...\n",
       "114996      114996       0  vineeta chadha wo jo ke time sab ne waste kiya...\n",
       "114997      114997       0       mujhe bhi do na caption credits akhil thakur\n",
       "114998      114998       0  people of balochistangilgit and pok have thank...\n",
       "114999      114999       1  jo it really should instead of doing this essa...\n",
       "\n",
       "[115000 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract text and labels\n",
    "train_sentences = train_df['tweets'].tolist()\n",
    "train_labels = train_df['labels'].tolist()\n",
    "\n",
    "test_sentences = test_df['tweets'].tolist()\n",
    "test_labels = test_df['labels'].tolist()\n",
    "\n",
    "validation_sentences = validation_df['tweets'].tolist()\n",
    "validation_labels = validation_df['labels'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'method': 'random',\n",
    "    'name': 'Transformer',\n",
    "    'metric': {\n",
    "        'name': 'bleu_score',\n",
    "        'goal': 'maximize',\n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {'value': 16},\n",
    "        'num_epochs': {'value': 100},\n",
    "        'learning_rate': {'values': [0.01, 0.005, 0.001]},\n",
    "        'embedding_dim': {'values': [64, 96, 128]},\n",
    "        'context_size': {'value': 64},\n",
    "        'dropout': {'values': [0.1, 0.2]},\n",
    "        'optimizer': {'values': ['Adam', 'RMSprop']},\n",
    "        'num_layers': {'values': [2, 3]},\n",
    "        'num_heads' : {'values' : [2, 4, 8]},\n",
    "        'model_path' : {'value' : './Transformer.pt'},\n",
    "        'drive_path' : {'value' : './'},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "def clean_sentences_eng(sentences):\n",
    "    # cleaned_sentences = []\n",
    "    # for sentence in sentences:\n",
    "    #     cleaned_sentence = sentence.lower().strip()\n",
    "    #     cleaned_sentence = re.sub(r'[^a-zA-Z\\s.!?\\']', '', cleaned_sentence)\n",
    "    #     cleaned_sentence = re.sub(r' +', ' ', cleaned_sentence)\n",
    "    #     cleaned_sentences.append(cleaned_sentence)\n",
    "    # return cleaned_sentences\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = sentence.lower().strip()\n",
    "        # cleaned_sentence = cleaned_sentence.replace('[^a-zA-Z0-9\\s.\\']',' ')\n",
    "        # cleaned_sentence = cleaned_sentence.replace(' +', ' ')\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "    return cleaned_sentences\n",
    "\n",
    "def clean_sentences_french(sentences):\n",
    "    # cleaned_sentences = []\n",
    "    # for sentence in sentences:\n",
    "    #     cleaned_sentence = sentence.lower().strip()\n",
    "    #     cleaned_sentence = re.sub(r\"[^a-zA-Zàâçéèêëîïôûùüÿñæœ.!?]+\", r\" \", cleaned_sentence)\n",
    "    #     cleaned_sentence = re.sub(r' +', ' ', cleaned_sentence)\n",
    "    #     cleaned_sentences.append(cleaned_sentence)\n",
    "    # return cleaned_sentences\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = sentence.lower().strip()\n",
    "        # cleaned_sentence = cleaned_sentence.replace('[^a-zA-Z0-9\\s.\\']',' ')\n",
    "        # cleaned_sentence = cleaned_sentence.replace(' +', ' ')\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eng_sentences = clean_sentences_eng(train_sentences)\n",
    "val_eng_sentences = clean_sentences_eng(validation_sentences)\n",
    "test_eng_sentences = clean_sentences_eng(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 104] Connection\n",
      "[nltk_data]     reset by peer>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "unique_english_train_words = set(nltk.word_tokenize(' '.join(train_eng_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57643\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_english_train_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(dataset):\n",
    "    vocab = list(set(dataset))\n",
    "    vocab.append('<unk>')\n",
    "    vocab.append('<pad>')\n",
    "    vocab.append('<sos>')\n",
    "    vocab.append('<eos>')\n",
    "    # vocab_size = len(vocab)\n",
    "\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "    return vocab, word_to_idx, idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab, english_word_to_idx, english_idx_to_word = get_vocabulary(unique_english_train_words)\n",
    "encoder_vocab_size = len(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=3, min_delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "        elif self.best_loss - loss > self.min_delta:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, labels, src_word_to_idx, context_size):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.context_size = context_size\n",
    "        self.src_word_to_idx = src_word_to_idx\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sentence = self.src_sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "        src_words = nltk.word_tokenize(src_sentence)\n",
    "\n",
    "        src_tokens = [self.src_word_to_idx['<sos>']] + [self.src_word_to_idx.get(word, self.src_word_to_idx['<unk>']) for word in src_words] + [self.src_word_to_idx['<eos>']]\n",
    "\n",
    "        len_src_tokens = len(src_tokens) if len(src_tokens) <= self.context_size else self.context_size\n",
    "\n",
    "        if len(src_tokens) >= self.context_size:\n",
    "            src_tokens = src_tokens[:self.context_size]\n",
    "        else:\n",
    "            src_tokens = src_tokens + [self.src_word_to_idx['<pad>']] * (self.context_size - len(src_tokens))\n",
    "\n",
    "        return torch.tensor(src_tokens), torch.tensor(label).float(), len_src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "def train(model: MyTransformer, criterion_train: nn.CrossEntropyLoss, criterion_val: nn.CrossEntropyLoss, optimizer: optim.Adam, num_epochs: int, data_loader_train: DataLoader, data_loader_val: DataLoader, es: EarlyStopping):\n",
    "    print(\"Hyperparameters set\")\n",
    "    print(\"Training started...\")\n",
    "\n",
    "    epoch_no = []\n",
    "    train_loss_epoch = []\n",
    "    val_loss_epoch = []\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_total = 0\n",
    "        validation_total = 0\n",
    "        train_correct = 0\n",
    "        validation_correct = 0\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        pbar = tqdm(enumerate(data_loader_train), total=len(data_loader_train))\n",
    "        for i, (src, labels, src_lens) in pbar:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src.to(device), src_lens.to(device))\n",
    "            output = output.squeeze(1)\n",
    "            labels = labels.to(device)\n",
    "            loss = criterion_train(output.to(device), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            predicted_class = torch.sigmoid(output) > 0.5\n",
    "            train_correct += torch.sum(predicted_class == labels).item()\n",
    "            train_total += len(predicted_class)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "            pbar.set_description(f'Epoch: {epoch + 1}, Train Loss: {train_loss / (i + 1):.4f}')\n",
    "\n",
    "        epoch_no.append(epoch + 1)\n",
    "        train_loss_epoch.append(train_loss / len(data_loader_train))\n",
    "\n",
    "        pbar = tqdm(enumerate(data_loader_val), total=len(data_loader_val))\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "\n",
    "        for i, (src, labels, src_lens) in pbar:\n",
    "            with torch.no_grad():\n",
    "\n",
    "                output = model(src.to(device), src_lens.to(device))\n",
    "                output = output.squeeze(1)\n",
    "                labels = labels.to(device)\n",
    "                loss = criterion_val(output.to(device), labels)\n",
    "                val_loss += loss.item()\n",
    "                predicted_class = torch.sigmoid(output) > 0.5\n",
    "                validation_correct += torch.sum(predicted_class == labels).item()\n",
    "                validation_total += len(predicted_class)\n",
    "\n",
    "                pbar.set_description(f'Epoch: {epoch + 1}, Val Loss: {val_loss / (i + 1):.4f}')\n",
    "\n",
    "        val_loss_epoch.append(val_loss / len(data_loader_val))\n",
    "        print(f'Epoch: {epoch + 1}, Train Loss: {train_loss_epoch[-1]:.4f}, Val Loss: {val_loss_epoch[-1]:.4f}')\n",
    "        print(f'Epoch: {epoch + 1}, Train Accuracy: {(train_correct/train_total):.4f}, Val Accuracy: {(validation_correct/validation_total):.4f}\\n\\n')\n",
    "    return output,epoch_no, train_loss_epoch, val_loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def test(model: MyTransformer, criterion_test: nn.CrossEntropyLoss, data_loader_test: DataLoader):\n",
    "    print(\"Testing...\")\n",
    "    model.to(device)\n",
    "    test_loss = 0\n",
    "    test_total = 0\n",
    "    test_correct = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    pbar = tqdm(enumerate(data_loader_test), total=len(data_loader_test))\n",
    "    \n",
    "    for i, (src, labels, src_lens) in pbar:\n",
    "        with torch.no_grad():\n",
    "            output = model(src.to(device), src_lens.to(device))\n",
    "            output = output.squeeze(1)\n",
    "            labels = labels.to(device)\n",
    "            loss = criterion_val(output.to(device), labels)\n",
    "            test_loss += loss.item()\n",
    "            predicted_class = torch.sigmoid(output) > 0.5\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted_class.cpu().numpy())\n",
    "            test_correct += torch.sum(predicted_class == labels).item()\n",
    "            test_total += len(predicted_class)\n",
    "    \n",
    "    accuracy = test_correct / test_total\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Hyperparameters\n",
    "\n",
    "context_size = config['parameters']['context_size']['value']\n",
    "model_path = config['parameters']['model_path']['value']\n",
    "batch_size = config['parameters']['batch_size']['value']\n",
    "drive_path = config['parameters']['drive_path']['value']\n",
    "num_epochs = config['parameters']['num_epochs']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = config\n",
    "learning_rate = 0.01\n",
    "embedding_dim = 64\n",
    "dropout = 0.1\n",
    "optimizer_name = 'Adam'\n",
    "num_layers = 2\n",
    "num_heads = 2\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "context_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(train_eng_sentences, train_labels, english_word_to_idx, context_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataset = TranslationDataset(val_eng_sentences, validation_labels, english_word_to_idx, context_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = TranslationDataset(test_eng_sentences, test_labels, english_word_to_idx, context_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MyTransformer(encoder_vocab_size, embedding_dim, context_size, num_layers, num_heads, dropout)\n",
    "criterion_train = nn.BCEWithLogitsLoss()\n",
    "criterion_val = nn.BCEWithLogitsLoss()\n",
    "criterion_test = nn.BCEWithLogitsLoss()\n",
    "optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
    "es = EarlyStopping(patience=3, min_delta=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters set\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.5035: 100%|██████████| 938/938 [00:20<00:00, 45.27it/s]\n",
      "Epoch: 1, Val Loss: 0.4452: 100%|██████████| 469/469 [00:04<00:00, 111.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.5035, Val Loss: 0.4452\n",
      "Epoch: 1, Train Accuracy: 0.7840, Val Accuracy: 0.8299\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.4375: 100%|██████████| 938/938 [00:20<00:00, 45.85it/s]\n",
      "Epoch: 2, Val Loss: 0.4432: 100%|██████████| 469/469 [00:04<00:00, 108.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.4375, Val Loss: 0.4432\n",
      "Epoch: 2, Train Accuracy: 0.8336, Val Accuracy: 0.8271\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 0.4191: 100%|██████████| 938/938 [00:20<00:00, 46.79it/s]\n",
      "Epoch: 3, Val Loss: 0.4408: 100%|██████████| 469/469 [00:04<00:00, 109.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 0.4191, Val Loss: 0.4408\n",
      "Epoch: 3, Train Accuracy: 0.8408, Val Accuracy: 0.8279\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 0.3973: 100%|██████████| 938/938 [00:19<00:00, 47.73it/s]\n",
      "Epoch: 4, Val Loss: 0.4348: 100%|██████████| 469/469 [00:04<00:00, 108.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 0.3973, Val Loss: 0.4348\n",
      "Epoch: 4, Train Accuracy: 0.8498, Val Accuracy: 0.8271\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 0.3822: 100%|██████████| 938/938 [00:20<00:00, 44.89it/s]\n",
      "Epoch: 5, Val Loss: 0.4147: 100%|██████████| 469/469 [00:04<00:00, 106.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 0.3822, Val Loss: 0.4147\n",
      "Epoch: 5, Train Accuracy: 0.8555, Val Accuracy: 0.8385\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 0.3741: 100%|██████████| 938/938 [00:19<00:00, 46.94it/s]\n",
      "Epoch: 6, Val Loss: 0.4345: 100%|██████████| 469/469 [00:03<00:00, 119.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 0.3741, Val Loss: 0.4345\n",
      "Epoch: 6, Train Accuracy: 0.8548, Val Accuracy: 0.8317\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 0.3690: 100%|██████████| 938/938 [00:19<00:00, 48.38it/s]\n",
      "Epoch: 7, Val Loss: 0.4353: 100%|██████████| 469/469 [00:04<00:00, 110.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 0.3690, Val Loss: 0.4353\n",
      "Epoch: 7, Train Accuracy: 0.8576, Val Accuracy: 0.8329\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 0.3583: 100%|██████████| 938/938 [00:18<00:00, 51.05it/s]\n",
      "Epoch: 8, Val Loss: 0.4723: 100%|██████████| 469/469 [00:04<00:00, 111.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 0.3583, Val Loss: 0.4723\n",
      "Epoch: 8, Train Accuracy: 0.8652, Val Accuracy: 0.8367\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train Loss: 0.3382: 100%|██████████| 938/938 [00:21<00:00, 44.16it/s]\n",
      "Epoch: 9, Val Loss: 0.4482: 100%|██████████| 469/469 [00:04<00:00, 109.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train Loss: 0.3382, Val Loss: 0.4482\n",
      "Epoch: 9, Train Accuracy: 0.8746, Val Accuracy: 0.8384\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 0.3259: 100%|██████████| 938/938 [00:16<00:00, 56.51it/s]\n",
      "Epoch: 10, Val Loss: 0.4449: 100%|██████████| 469/469 [00:04<00:00, 110.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 0.3259, Val Loss: 0.4449\n",
      "Epoch: 10, Train Accuracy: 0.8783, Val Accuracy: 0.8378\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output,epoch_no, train_loss_epoch, val_loss_epoch = train(model, criterion_train, criterion_val, optimizer,\n",
    "                                          num_epochs, train_dataloader, val_dataloader, es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:03<00:00, 138.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8433\n",
      "F1 Score: 0.8322\n",
      "Precision: 0.9050\n",
      "Recall: 0.7703\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, criterion_test, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
