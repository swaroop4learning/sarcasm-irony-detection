{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Irony classification using Bi directional encoder transformers architecture\"\"\""
      ],
      "metadata": {
        "id": "58UhBT3Uj7C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9Dc8AvzfzdI",
        "outputId": "82aba3b1-bc7c-46a5-9ca8-7c2c4edecb5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "lpDpVF46f_bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use StringIO to simulate a file object\n",
        "train_df = pd.read_csv('gdrive/My Drive/anlp_project/SemEval2018-T3-train-taskA_emoji.txt', sep='\\t')"
      ],
      "metadata": {
        "id": "5xRVOT_EgCFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('gdrive/My Drive/anlp_project/SemEval2018-T3_gold_test_taskA_emoji.txt', sep='\\t')"
      ],
      "metadata": {
        "id": "lLQt1L2jgGCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#very basic preprocessing\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "\n",
        "    # replace possible sarcasm expressions with possiblity\n",
        "    re.sub(r'\\. \\.\\.', ' possibility ', tweet)\n",
        "    re.sub(r'\\. \\.\\.\\.', ' possibility ', tweet)\n",
        "\n",
        "     # Replace patterns like \"250,000\", \"3,600\" with \"NUM\"\n",
        "    #text = re.sub(r'\\b\\d{1,3}(?:,\\d{3})*(?!\\d)', 'num', tweet)\n",
        "\n",
        "    # Replace urls\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", 'URL', tweet, flags=re.MULTILINE)\n",
        "\n",
        "    #replace numbers\n",
        "    #tweet = re.sub(r'^\\d+$', 'num', tweet)\n",
        "    tweet = re.sub(r'\\b\\d+\\b', 'num', tweet)\n",
        "\n",
        "\n",
        "    return tweet\n"
      ],
      "metadata": {
        "id": "2l6EryRggLD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text_prep'] = train_df['Tweet text'].apply(lambda x: preprocess_tweet(x))"
      ],
      "metadata": {
        "id": "hWYnsI1Ig5KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['text_prep'] = test_df['Tweet text'].apply(lambda x: preprocess_tweet(x))"
      ],
      "metadata": {
        "id": "WTFtG8pHgwJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tweets = train_df['text_prep'].tolist()"
      ],
      "metadata": {
        "id": "7fWKvTb2hyyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "# Use the pre-tokenizer responsible for splitting the input into words\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "# Train the tokenizer\n",
        "trainer = trainers.BpeTrainer(vocab_size=50000, show_progress=True)\n",
        "tokenizer.train_from_iterator(train_tweets, trainer=trainer)  # train from iterator\n",
        "\n",
        "# Once it's trained, save it\n",
        "tokenizer.save('trained_tokenizer.json')\n"
      ],
      "metadata": {
        "id": "31lGSsNjhF4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, tweets, labels, tokenizer, max_len):\n",
        "        self.tweets = tweets\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tweet = self.tweets[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode(tweet)\n",
        "        tokens = encoding.ids\n",
        "\n",
        "        # Truncate or pad the tokens\n",
        "        if len(tokens) > self.max_len:\n",
        "            tokens = tokens[:self.max_len]\n",
        "        else:\n",
        "            tokens += [0] * (self.max_len - len(tokens))\n",
        "\n",
        "        # The attention mask should have 1 for real tokens and 0 for padding\n",
        "        attention_mask = [1 if token_id > 0 else 0 for token_id in tokens]\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(tokens),\n",
        "            'attention_mask': torch.tensor(attention_mask),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Example\n",
        "tweets = train_tweets\n",
        "labels = train_df['Label'].tolist()  # 0 for non-ironic and 1 for ironic\n",
        "dataset = TweetDataset(tweets, labels, tokenizer, max_len=230)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "dY-JRY9BiLMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweets = test_df['text_prep'].tolist()\n",
        "test_labels = test_df['Label'].tolist()\n",
        "test_dataset = TweetDataset(test_tweets, test_labels, tokenizer, max_len=230)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "E2GlJdB5qcan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertModel\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
        "from torch.nn import Linear\n",
        "\n",
        "class TweetClassifier(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(TweetClassifier, self).__init__()\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.encoder = BertModel(config)\n",
        "\n",
        "        # Positional embeddings (they are by default included in the TransformerModel in Hugging Face)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = Linear(config.hidden_size, 2)  # Binary classification\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Pass through transformer\n",
        "        outputs: BaseModelOutputWithPoolingAndCrossAttentions = self.encoder(\n",
        "            input_ids, attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Use CLS token for classification task\n",
        "        cls_output = outputs.last_hidden_state[:, 0]\n",
        "\n",
        "        # Pass through classification head\n",
        "        logits = self.classifier(cls_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# Configuration for the transformer\n",
        "config = BertConfig(\n",
        "    vocab_size=tokenizer.get_vocab_size(),\n",
        "    hidden_size=256,\n",
        "    num_attention_heads=4,\n",
        "    num_hidden_layers=2,\n",
        "    intermediate_size=1024,\n",
        ")\n",
        "\n",
        "model = TweetClassifier(config)\n"
      ],
      "metadata": {
        "id": "djERppbNikW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZbCK6wzjhBJ",
        "outputId": "08596534-29af-47d4-8868-da230ca2103d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TweetClassifier(\n",
              "  (encoder): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(25280, 256, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 256)\n",
              "      (token_type_embeddings): Embedding(2, 256)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-1): 2 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Assuming you have set up your model, optimizer, and loss criterion\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    train_probabilities = []\n",
        "    total_loss = 0.0\n",
        "    for batch in loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the predictions\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        total_loss += loss.item()\n",
        "        probs = torch.nn.functional.softmax(outputs.detach(), dim=1)[:, 1].cpu().numpy()\n",
        "        train_probabilities.extend(probs)\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    f1 = f1_score(all_labels, all_predictions)\n",
        "    # Compute ROC AUC score for the positive class\n",
        "    train_auc = roc_auc_score(all_labels, train_probabilities)\n",
        "    #auc = roc_auc_score(all_labels, [output_probs[1] for output_probs in torch.nn.functional.softmax(outputs.detach(), dim=1).cpu().numpy()])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} Loss: {avg_loss:.4f} Accuracy: {accuracy:.4f} F1: {f1:.4f}, AUC: {train_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_e9lDLYjcTl",
        "outputId": "7cdc95d6-d3ff-4c85-9504-c2220e8f6cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 Loss: 0.6887 Accuracy: 0.5520 F1: 0.5433, AUC: 0.5762\n",
            "Epoch 2/3 Loss: 0.5934 Accuracy: 0.6717 F1: 0.6656, AUC: 0.7435\n",
            "Epoch 3/3 Loss: 0.3519 Accuracy: 0.8522 F1: 0.8508, AUC: 0.9262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "test_probabilities = []\n",
        "with torch.no_grad():\n",
        "  for batch in test_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    predictions = torch.argmax(outputs, dim=1)\n",
        "    test_predictions.extend(predictions.cpu().numpy())\n",
        "    test_labels.extend(labels.cpu().numpy())\n",
        "    probs = torch.nn.functional.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
        "    test_probabilities.extend(probs)\n",
        "\n",
        "  test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "  test_f1 = f1_score(test_labels, test_predictions)\n",
        "  # Compute ROC AUC score for the positive class for the test set\n",
        "  test_auc = roc_auc_score(test_labels, test_probabilities)\n",
        "  print(f\"Test Accuracy: {test_accuracy:.4f} Test F1: {test_f1:.4f} Test AUC: {test_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N7K6VNBmEQV",
        "outputId": "bfa650e8-dd83-49a6-9a73-51d0915ebcb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6429 Test F1: 0.5018 Test AUC: 0.6619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = 'gdrive/My Drive/anlp_project/transformers/nonbert_model_weights.pth'\n",
        "tokenizer_save_path = 'gdrive/My Drive/anlp_project/transformers/tokenizer.json'\n",
        "\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "# Save tokenizer\n",
        "tokenizer.save(tokenizer_save_path)\n"
      ],
      "metadata": {
        "id": "yND1JeJTroT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading code\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "# Load model\n",
        "model = TweetClassifier(config)\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.eval()\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = Tokenizer.from_file(tokenizer_save_path)"
      ],
      "metadata": {
        "id": "WbKIiZBAuYU3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}