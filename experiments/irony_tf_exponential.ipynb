{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "doGwTDfl4kh8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y-Q7QAU4ng4",
        "outputId": "a47bd5f9-e1e0-4a69-ca3f-532268143970"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "n6XJMdFj4pzl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('gdrive/My Drive/anlp_project/train_preprocessed_df_setfit.csv')\n",
        "test_df = pd.read_csv('gdrive/My Drive/anlp_project/test_preprocessed_df_setfit.csv')"
      ],
      "metadata": {
        "id": "YKJxYRQ14rL8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPjmOdOG5nKn",
        "outputId": "9e96a9b0-a771-4fbe-ced6-4b02b700d182"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'Tweet index', 'Label', 'Tweet text', 'text_prep',\n",
              "       'tweet_tokens', 'tweet_prep'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1yM_Z0t41Rr",
        "outputId": "1394ccf4-a3d7-419f-e438-6b9c505293d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NUtkJsbQ4dXl"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers import DebertaTokenizer, DebertaModel\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Define the ExponentialPositionalEncoding\n",
        "class ExponentialPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super(ExponentialPositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        reversed_position = max_len - position\n",
        "        div_term = torch.exp(-0.01 * reversed_position)\n",
        "        self.encoding = reversed_position * div_term\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.encoding[:, :x.size(1), :].to(x.device)\n",
        "        return x\n",
        "\n",
        "# Create a custom BERT model with Exponential Positional Encoding\n",
        "class CustomBERTModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(CustomBERTModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.positional_encoding = ExponentialPositionalEncoding(d_model=768) # BERT base has 768 dims\n",
        "        self.classifier = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        last_hidden_state = outputs[0] # shape: (batch_size, seq_len, d_model)\n",
        "        # Apply the positional encoding\n",
        "        encoded_output = self.positional_encoding(last_hidden_state)\n",
        "        # Take the [CLS] embedding for classification\n",
        "        cls_output = encoded_output[:, 0, :]\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "# Example usage\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = CustomBERTModel(num_labels=2) # Binary classification (irony or not)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom RoBERTa model with Exponential Positional Encoding\n",
        "class CustomRobertaModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(CustomRobertaModel, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.positional_encoding = ExponentialPositionalEncoding(d_model=768) # RoBERTa base has 768 dims\n",
        "        self.classifier = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs[0] # shape: (batch_size, seq_len, d_model)\n",
        "        # Apply the positional encoding\n",
        "        encoded_output = self.positional_encoding(last_hidden_state)\n",
        "        # Take the [CLS] embedding for classification\n",
        "        cls_output = encoded_output[:, 0, :]\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "XKd42p3GE39W"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom RoBERTa model with Exponential Positional Encoding\n",
        "class CustomDebertaModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(CustomDebertaModel, self).__init__()\n",
        "        self.roberta = DebertaModel.from_pretrained('microsoft/deberta-base')\n",
        "        self.positional_encoding = ExponentialPositionalEncoding(d_model=768) # Deberta base has 768 dims\n",
        "        self.classifier = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs[0] # shape: (batch_size, seq_len, d_model)\n",
        "        # Apply the positional encoding\n",
        "        encoded_output = self.positional_encoding(last_hidden_state)\n",
        "        # Take the [CLS] embedding for classification\n",
        "        cls_output = encoded_output[:, 0, :]\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "WWAGtMUIGCsi"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "text = \"This is an ironic statement!\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT0x_kYg49Hh",
        "outputId": "0120caac-6bed-4d9c-a838-f947ac9ee83a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.7405, 0.6551]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset, random_split"
      ],
      "metadata": {
        "id": "kXT8Q4Bf5XxT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the tweets\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "inputs = tokenizer(list(train_df['tweet_prep']), padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
        "input_ids = inputs['input_ids']\n",
        "attention_masks = inputs['attention_mask']\n",
        "labels = torch.tensor(train_df['Label'])\n",
        "\n",
        "# Create a DataLoader\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "validation_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "yP8LeBHw5glR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loaded_bert = BertModel.from_pretrained(\"gdrive/My Drive/anlp_project/custom_bert_model\")\n",
        "# loaded_tokenizer = BertTokenizer.from_pretrained(\"gdrive/My Drive/anlp_project/custom_bert_tokenizer\")\n",
        "# loaded_bert.to('cuda') # Move to GPU if needed"
      ],
      "metadata": {
        "id": "Uk6KY6ejUilv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = tokenizer(list(test_df['tweet_prep']), padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
        "test_input_ids = test_inputs['input_ids']\n",
        "test_attention_masks = test_inputs['attention_mask']\n",
        "test_labels = torch.tensor(test_df['Label'])"
      ],
      "metadata": {
        "id": "8thnK5Ul8ukX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "XTruZcfu88aa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "# Initialize the custom BERT model\n",
        "model = CustomBERTModel(num_labels=2).to('cuda')\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "epochs = 4\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbEP5dic57qP",
        "outputId": "1923b4cb-becd-41c3-ad2c-88d276e0834b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[0].to('cuda')\n",
        "        attention_mask = batch[1].to('cuda')\n",
        "        labels = batch[2].to('cuda')\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch: {epoch + 1}, Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch[0].to('cuda')\n",
        "        attention_mask = batch[1].to('cuda')\n",
        "        labels = batch[2].to('cuda')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        labels = labels.to('cpu').numpy()\n",
        "        total_eval_accuracy += (logits.argmax(axis=1) == labels).mean()\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(f\"Epoch: {epoch + 1}, Validation Accuracy: {avg_val_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYuL650z6CJa",
        "outputId": "10ade5b3-deda-4349-9308-6bb5510a1505"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.6378\n",
            "Epoch: 1, Validation Accuracy: 0.6484\n",
            "Epoch: 2, Training Loss: 0.4857\n",
            "Epoch: 2, Validation Accuracy: 0.6905\n",
            "Epoch: 3, Training Loss: 0.3010\n",
            "Epoch: 3, Validation Accuracy: 0.7091\n",
            "Epoch: 4, Training Loss: 0.1626\n",
            "Epoch: 4, Validation Accuracy: 0.7091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "# Place the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Lists to store model predictions and true labels\n",
        "all_logits = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.to('cuda') for t in batch)\n",
        "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
        "\n",
        "        # Get model outputs\n",
        "        outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "\n",
        "        # Store outputs and true labels\n",
        "        all_logits.append(outputs.cpu())\n",
        "        true_labels.append(inputs['labels'].cpu())\n",
        "\n",
        "# Convert the lists into tensors\n",
        "all_logits = torch.cat(all_logits, dim=0)\n",
        "true_labels = torch.cat(true_labels, dim=0)\n",
        "\n",
        "# Compute softmax over logits to get probabilities of class 1\n",
        "probs = torch.nn.functional.softmax(all_logits, dim=1)[:, 1]\n",
        "\n",
        "# Compute AUC\n",
        "auc = roc_auc_score(true_labels, probs)\n",
        "\n",
        "# Compute accuracy\n",
        "preds = torch.argmax(all_logits, dim=1)\n",
        "accuracy = accuracy_score(true_labels, preds)\n",
        "\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRKd6YYS6Xlj",
        "outputId": "33d32b91-6461-49a0-9068-f463c2549a31"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.7674\n",
            "Accuracy: 0.6939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "precision_score(true_labels, preds), recall_score(true_labels, preds), f1_score(true_labels, preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WIPnlRU9vg7",
        "outputId": "eb85617f-9c41-4243-f58b-2ce620a2cdc7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5907928388746803, 0.7427652733118971, 0.6581196581196581)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert uncased metrics\n",
        "Accuracy - 0.7194\n",
        "Precision - 0.60\n",
        "Recall - 0.816\n",
        "F1 - 0.69\n",
        "AUC - 0.81"
      ],
      "metadata": {
        "id": "HvdFPkX0BQWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = \"gdrive/My Drive/anlp_project/custom_bert_model\"\n",
        "model.bert.save_pretrained(model_save_path)\n",
        "tokenizer_save_path = \"gdrive/My Drive/anlp_project/custom_bert_tokenizer\"\n",
        "tokenizer.save_pretrained(tokenizer_save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClS76K5q-fO-",
        "outputId": "44e4a548-1205-4bd5-ad39-4bac7b2bb8b0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gdrive/My Drive/anlp_project/custom_bert_tokenizer/tokenizer_config.json',\n",
              " 'gdrive/My Drive/anlp_project/custom_bert_tokenizer/special_tokens_map.json',\n",
              " 'gdrive/My Drive/anlp_project/custom_bert_tokenizer/vocab.txt',\n",
              " 'gdrive/My Drive/anlp_project/custom_bert_tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HhmIaem3Av4y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}