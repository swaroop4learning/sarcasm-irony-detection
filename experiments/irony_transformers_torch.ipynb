{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Irony classification using transformers\"\"\""
      ],
      "metadata": {
        "id": "72fv9SqCjQ5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maANiDuI6qi3",
        "outputId": "7bd5ffc0-fcf8-47e3-9510-9704f3f24c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "RZ3CYmZa6lE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use StringIO to simulate a file object\n",
        "train_df = pd.read_csv('gdrive/My Drive/anlp_project/SemEval2018-T3-train-taskA_emoji.txt', sep='\\t')\n",
        "test_df = pd.read_csv('gdrive/My Drive/anlp_project/SemEval2018-T3_gold_test_taskA_emoji.txt', sep='\\t')"
      ],
      "metadata": {
        "id": "oBcBiz5P6wf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#very basic preprocessing\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "\n",
        "    # replace possible sarcasm expressions with possiblity\n",
        "    re.sub(r'\\. \\.\\.', ' possibility ', tweet)\n",
        "    re.sub(r'\\. \\.\\.\\.', ' possibility ', tweet)\n",
        "\n",
        "     # Replace patterns like \"250,000\", \"3,600\" with \"NUM\"\n",
        "    #text = re.sub(r'\\b\\d{1,3}(?:,\\d{3})*(?!\\d)', 'num', tweet)\n",
        "\n",
        "    # Replace urls\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", 'URL', tweet, flags=re.MULTILINE)\n",
        "\n",
        "    #replace numbers\n",
        "    #tweet = re.sub(r'^\\d+$', 'num', tweet)\n",
        "    tweet = re.sub(r'\\b\\d+\\b', 'num', tweet)\n",
        "\n",
        "\n",
        "    return tweet\n"
      ],
      "metadata": {
        "id": "lK5XiH6B61d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text_prep'] = train_df['Tweet text'].apply(lambda x: preprocess_tweet(x))\n",
        "test_df['text_prep'] = test_df['Tweet text'].apply(lambda x: preprocess_tweet(x))"
      ],
      "metadata": {
        "id": "VnYHkivL65M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tweets = train_df['text_prep'].tolist()"
      ],
      "metadata": {
        "id": "wGPsCZrh6-rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "# Load tokenizer\n",
        "tokenizer_save_path = 'gdrive/My Drive/anlp_project/transformers/tokenizer.json'\n",
        "tokenizer = Tokenizer.from_file(tokenizer_save_path)"
      ],
      "metadata": {
        "id": "0JaOGdOB6lwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, tweets, labels, tokenizer, max_len):\n",
        "        self.tweets = tweets\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tweet = self.tweets[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode(tweet)\n",
        "        tokens = encoding.ids\n",
        "\n",
        "        # Truncate or pad the tokens\n",
        "        if len(tokens) > self.max_len:\n",
        "            tokens = tokens[:self.max_len]\n",
        "        else:\n",
        "            tokens += [0] * (self.max_len - len(tokens))\n",
        "\n",
        "        # The attention mask should have 1 for real tokens and 0 for padding\n",
        "        attention_mask = [1 if token_id > 0 else 0 for token_id in tokens]\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(tokens),\n",
        "            'attention_mask': torch.tensor(attention_mask),\n",
        "            'label': torch.tensor(label, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Example\n",
        "tweets = train_tweets\n",
        "labels = train_df['Label'].tolist()  # 0 for non-ironic and 1 for ironic\n",
        "dataset = TweetDataset(tweets, labels, tokenizer, max_len=230)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "-vYaoZq16l7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweets = test_df['text_prep'].tolist()\n",
        "test_labels = test_df['Label'].tolist()\n",
        "test_dataset = TweetDataset(test_tweets, test_labels, tokenizer, max_len=230)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "YtYarzxJ6mDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ni_9Hp857LP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Modified: Added a pooling layer and changed the output size\n",
        "        #self.pooling = nn.AdaptiveAvgPool1d(1)\n",
        "        self.linear = nn.Linear(d_model, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
        "        src = src.permute(1, 0)  # Make it [seq_len, batch_size]\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "\n",
        "        # Modified: Added pooling to obtain a fixed-size representation for the entire sequence\n",
        "        #pooled_output = self.pooling(output.permute(1, 2, 0)).squeeze(-1)\n",
        "        pooled_output = output.mean(dim=0)\n",
        "        logits = self.linear(pooled_output).squeeze(-1)\n",
        "        output = self.sigmoid(logits)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "bne9O1ew6clF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = tokenizer.get_vocab_size()  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
        "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
        "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
        "dropout = 0.1  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ],
      "metadata": {
        "id": "frwNsQx67Njq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqY2a5Fi7TvO",
        "outputId": "807db32c-6cfb-46ec-fd14-aca10eb28a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (embedding): Embedding(25280, 200)\n",
              "  (linear): Linear(in_features=200, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from torch.nn import BCELoss\n",
        "\n",
        "# Assuming you have set up your model, optimizer, and loss criterion\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "criterion = BCELoss().to(device)\n",
        "\n",
        "\n",
        "epochs = 6\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    train_probabilities = []\n",
        "    total_loss = 0.0\n",
        "    for batch in loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        #attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids).squeeze()\n",
        "        #outputs = outputs[-1].squeeze(dim=-1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the predictions\n",
        "        predictions = (outputs > 0.5).int()\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        total_loss += loss.item()\n",
        "        #probs = torch.nn.functional.softmax(outputs.detach(), dim=1)[:, 1].cpu().numpy()\n",
        "        train_probabilities.extend(outputs.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    f1 = f1_score(all_labels, all_predictions)\n",
        "    # Compute ROC AUC score for the positive class\n",
        "    train_auc = roc_auc_score(all_labels, train_probabilities)\n",
        "    #auc = roc_auc_score(all_labels, [output_probs[1] for output_probs in torch.nn.functional.softmax(outputs.detach(), dim=1).cpu().numpy()])\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} Loss: {avg_loss:.4f} Accuracy: {accuracy:.4f} F1: {f1:.4f}, AUC: {train_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkJMtOkw7WPp",
        "outputId": "49817dfb-76ab-4da3-9864-c7af482a423d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6 Loss: 0.6880 Accuracy: 0.5426 F1: 0.5584, AUC: 0.5570\n",
            "Epoch 2/6 Loss: 0.6416 Accuracy: 0.6369 F1: 0.6419, AUC: 0.6808\n",
            "Epoch 3/6 Loss: 0.5629 Accuracy: 0.7066 F1: 0.7005, AUC: 0.7836\n",
            "Epoch 4/6 Loss: 0.4186 Accuracy: 0.8019 F1: 0.8007, AUC: 0.8909\n",
            "Epoch 5/6 Loss: 0.2465 Accuracy: 0.9039 F1: 0.9038, AUC: 0.9632\n",
            "Epoch 6/6 Loss: 0.1371 Accuracy: 0.9492 F1: 0.9489, AUC: 0.9877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout)\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvaHkJ5tFnv-",
        "outputId": "8575b96e-0002-42fa-9725-5fcf2e8d68ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (embedding): Embedding(25280, 200)\n",
              "  (linear): Linear(in_features=200, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "test_probabilities = []\n",
        "with torch.no_grad():\n",
        "  for batch in test_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    #attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "\n",
        "    outputs = model(input_ids).squeeze()\n",
        "\n",
        "    predictions = (outputs > 0.5).int()\n",
        "    test_predictions.extend(predictions.cpu().numpy())\n",
        "    test_labels.extend(labels.cpu().numpy())\n",
        "    #probs = torch.nn.functional.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
        "    test_probabilities.extend(outputs.detach().cpu().numpy())\n",
        "\n",
        "  test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "  test_f1 = f1_score(test_labels, test_predictions)\n",
        "  # Compute ROC AUC score for the positive class for the test set\n",
        "  test_auc = roc_auc_score(test_labels, test_probabilities)\n",
        "  print(f\"Test Accuracy: {test_accuracy:.4f} Test F1: {test_f1:.4f} Test AUC: {test_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zIIpHe-82ft",
        "outputId": "16674c2e-35db-446c-9bd9-5704cd534257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6454 Test F1: 0.6128 Test AUC: 0.7028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = 'gdrive/My Drive/anlp_project/transformers/transformer_model_weights.pth'\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), model_save_path)"
      ],
      "metadata": {
        "id": "ItLcsaCcElvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zy77v4r8FMkj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}