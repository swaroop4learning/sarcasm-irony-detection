{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Irony classification using BiLSTM attention model with both fasttext and word2vec embeddings. Along with k-fold and hyperparameter tuning\"\"\""
      ],
      "metadata": {
        "id": "ChVU2rwIjhOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFYTqE0ltphJ",
        "outputId": "19a1d6f0-03ce-4053-f86d-f2b17bc25aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-nnssQVXvJMW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('gdrive/My Drive/anlp_project/train_preprocessed_df_fasttext.csv')\n",
        "test_df = pd.read_csv('gdrive/My Drive/anlp_project/test_preprocessed_df_fasttext.csv')"
      ],
      "metadata": {
        "id": "pZvHHa8rvMp6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the embeddings\n",
        "with open('gdrive/My Drive/anlp_project/my_embeddings_fasttext.pkl', 'rb') as file:\n",
        "    embeddings_dict = pickle.load(file)\n",
        "\n",
        "# Example: To get the embedding of the word 'hello'\n",
        "embedding_hello = embeddings_dict.get('hello', None)\n"
      ],
      "metadata": {
        "id": "5QtaCfbXvlhL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_hello.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRRt7dLov9Rl",
        "outputId": "950d2472-3fed-4bf1-b928-09e69a892ec2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('gdrive/My Drive/anlp_project/my_emojis_fasttext.pkl', 'rb') as file:\n",
        "    emojis_dict = pickle.load(file)"
      ],
      "metadata": {
        "id": "BRwn2pzgwaiG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "for emoji, embedding in emojis_dict.items():\n",
        "    embedding_tensor = torch.tensor(embedding)\n",
        "    padded_embedding = torch.cat([embedding_tensor, torch.zeros(100)])\n",
        "    emojis_dict[emoji] = padded_embedding"
      ],
      "metadata": {
        "id": "BVkfq2YQztCw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtags_corpus = [key for key in embeddings_dict.keys() if key.startswith('#')]"
      ],
      "metadata": {
        "id": "N4rGGYH4vd28"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(hashtags_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMpQuxvdw4Ox",
        "outputId": "22aaa63e-9db7-4bc5-a3bd-17ac99161cd4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2769"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, dataframe, twitter_word2vec, emoji_vec, max_len):\n",
        "        self.dataframe = dataframe\n",
        "        self.twitter_word2vec = twitter_word2vec\n",
        "        self.emoji_vec = emoji_vec\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tweet_tokens = self.dataframe.iloc[idx]['tweet_tokens']\n",
        "\n",
        "        # Generate embeddings for tokens\n",
        "        embeddings = []\n",
        "        for token in tweet_tokens:\n",
        "            if token in self.twitter_word2vec:\n",
        "                embeddings.append(torch.tensor(self.twitter_word2vec[token]))\n",
        "            elif token in self.emoji_vec:\n",
        "                embeddings.append(self.emoji_vec[token])\n",
        "            else:\n",
        "                # Handle unknown words or use a predefined UNK vector\n",
        "                embeddings.append(torch.zeros(400))\n",
        "\n",
        "        # Pad to max_len\n",
        "        while len(embeddings) < self.max_len:\n",
        "            embeddings.append(torch.zeros(400))\n",
        "\n",
        "        # If the sequence is longer than max_len, truncate it\n",
        "        embeddings = embeddings[:self.max_len]\n",
        "\n",
        "        embeddings = torch.stack(embeddings)\n",
        "\n",
        "        label = torch.tensor(self.dataframe.iloc[idx]['Label'])\n",
        "\n",
        "        return embeddings, label\n"
      ],
      "metadata": {
        "id": "dSNyV214w-ek"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TweetDataset_fasttext(Dataset):\n",
        "    def __init__(self, dataframe, twitter_fasttext, emoji_vec, max_len):\n",
        "        self.dataframe = dataframe\n",
        "        self.twitter_fasttext = twitter_fasttext\n",
        "        self.emoji_vec = emoji_vec\n",
        "        self.max_len = max_len\n",
        "        self.unk = 'UNK'\n",
        "        self.hashtag = '#hashtag'\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tweet_tokens = self.dataframe.iloc[idx]['tweet_tokens']\n",
        "\n",
        "        # Generate embeddings for tokens\n",
        "        embeddings = []\n",
        "        for token in tweet_tokens:\n",
        "            if token in self.twitter_fasttext:\n",
        "                embeddings.append(torch.tensor(self.twitter_fasttext[token]))\n",
        "            elif token in self.emoji_vec:\n",
        "                embeddings.append(self.emoji_vec[token])\n",
        "            elif token.startswith('#'):\n",
        "                # Handles unknown hashtags\n",
        "                embeddings.append(torch.tensor(self.twitter_fasttext[self.hashtag]))\n",
        "            else:\n",
        "                # Handle unknown words or use a predefined UNK vector\n",
        "                embeddings.append(torch.tensor(self.twitter_fasttext[self.unk]))\n",
        "\n",
        "        # Pad to max_len\n",
        "        while len(embeddings) < self.max_len:\n",
        "            embeddings.append(torch.zeros(400))\n",
        "\n",
        "        # If the sequence is longer than max_len, truncate it\n",
        "        embeddings = embeddings[:self.max_len]\n",
        "\n",
        "        embeddings = torch.stack(embeddings)\n",
        "\n",
        "        label = torch.tensor(self.dataframe.iloc[idx]['Label'])\n",
        "\n",
        "        return embeddings, label\n"
      ],
      "metadata": {
        "id": "0gposiIkmosY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VLiBGVx3xhkt",
        "outputId": "6587ab6f-ac5d-41d1-cbad-e0ad53e3cce6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Tweet index  Label                                         Tweet text  \\\n",
              "0            1      1  Sweet United Nations video. Just in time for C...   \n",
              "1            2      1  @mrdahl87 We are rumored to have talked to Erv...   \n",
              "2            3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n",
              "3            4      0                3 episodes left I'm dying over here   \n",
              "4            5      1  I can't breathe! was chosen as the most notabl...   \n",
              "\n",
              "                                           text_prep  \\\n",
              "0  Sweet United Nations video. Just in time for C...   \n",
              "1  @mrdahl87 We are rumored to have talked to Erv...   \n",
              "2  Hey there! Nice to see you Minnesota/ND Winter...   \n",
              "3              NUM episodes left I'm dying over here   \n",
              "4  I can't breathe! was chosen as the most notabl...   \n",
              "\n",
              "                                        tweet_tokens  \n",
              "0  ['sweet', 'united', 'nations', 'video', 'punct...  \n",
              "1  ['we', 'are', 'rumored', 'to', 'have', 'talked...  \n",
              "2  ['hey', 'there', '!', 'nice', 'to', 'see', 'yo...  \n",
              "3  ['num', 'episodes', 'left', \"i'm\", 'dying', 'o...  \n",
              "4  ['i', \"can't\", 'breathe', '!', 'was', 'chosen'...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7efad0a0-7855-415e-ba9e-cb737af4bbcb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet index</th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet text</th>\n",
              "      <th>text_prep</th>\n",
              "      <th>tweet_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Sweet United Nations video. Just in time for C...</td>\n",
              "      <td>Sweet United Nations video. Just in time for C...</td>\n",
              "      <td>['sweet', 'united', 'nations', 'video', 'punct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
              "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
              "      <td>['we', 'are', 'rumored', 'to', 'have', 'talked...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
              "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
              "      <td>['hey', 'there', '!', 'nice', 'to', 'see', 'yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3 episodes left I'm dying over here</td>\n",
              "      <td>NUM episodes left I'm dying over here</td>\n",
              "      <td>['num', 'episodes', 'left', \"i'm\", 'dying', 'o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
              "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
              "      <td>['i', \"can't\", 'breathe', '!', 'was', 'chosen'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7efad0a0-7855-415e-ba9e-cb737af4bbcb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7efad0a0-7855-415e-ba9e-cb737af4bbcb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7efad0a0-7855-415e-ba9e-cb737af4bbcb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0cb6fdba-9f17-4c81-9dbe-1fa1c43ab639\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0cb6fdba-9f17-4c81-9dbe-1fa1c43ab639')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0cb6fdba-9f17-4c81-9dbe-1fa1c43ab639 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine max_len (e.g., based on the 95th percentile of tweet lengths)\n",
        "max_len = int(train_df['tweet_tokens'].apply(len).quantile(0.95))"
      ],
      "metadata": {
        "id": "GQdEaD7HxgBq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gvGkAMAxuSA",
        "outputId": "7b122887-cc39-45b0-8690-d0bc02de9126"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage:\n",
        "train_dataset = TweetDataset_fasttext(train_df, embeddings_dict, emojis_dict, max_len=max_len)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "mXCh8Axjxvll"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TweetDataset_fasttext(test_df, embeddings_dict, emojis_dict, max_len=max_len)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "Jh1ywczK0Rye"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_prob=0.1):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers,\n",
        "                            batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
        "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, input_dim)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        # We take the output from the final time step\n",
        "        final_output = lstm_out[:, -1, :]\n",
        "        out = self.fc(final_output)\n",
        "        return out\n",
        "\n",
        "# Example usage:\n",
        "model = BiLSTM(input_dim=400, hidden_dim=256, num_layers=2, output_dim=1)  # Assuming binary classification for irony detection"
      ],
      "metadata": {
        "id": "C4cadPL_x2Oj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BiLSTM_Attention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_prob=0.0):\n",
        "        super(BiLSTM_Attention, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers,\n",
        "                            batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
        "        self.attention_linear = nn.Linear(2 * hidden_dim, 1)  # Attention scoring function\n",
        "        self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, input_dim)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        att_scores = self.attention_linear(lstm_out)  # (batch_size, seq_len, 1)\n",
        "        att_scores = att_scores.squeeze(-1)  # (batch_size, seq_len)\n",
        "        att_weights = F.softmax(att_scores, dim=1).unsqueeze(2)  # (batch_size, seq_len, 1)\n",
        "\n",
        "        # Calculate weighted sum of LSTM outputs\n",
        "        weighted_sum = torch.sum(lstm_out * att_weights, dim=1)  # (batch_size, 2*hidden_dim)\n",
        "\n",
        "        out = self.fc(weighted_sum)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Example usage:\n",
        "model = BiLSTM_Attention(input_dim=400, hidden_dim=256, num_layers=2, output_dim=1)  # Assuming binary classification for irony detection\n"
      ],
      "metadata": {
        "id": "wztvIroP9GmX"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # learning rate of 0.001\n"
      ],
      "metadata": {
        "id": "uICsFEa3x99P"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyVqJNHv3n7p",
        "outputId": "56c82338-b7d2-4175-c0f6-925367943649"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM_Attention(\n",
              "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
              "  (attention_linear): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install optuna"
      ],
      "metadata": {
        "id": "ydfNl9tO1_99"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score"
      ],
      "metadata": {
        "id": "SPJW7Lt44bFI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_training_loop(model, optimizer, train_dataloader, test_dataloader, criterion):\n",
        "  num_epochs = 15  # Example number of epochs\n",
        "  best_test_f1 = 0\n",
        "  epochs_without_improvement = 0\n",
        "  patience = 3\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      all_predictions = []\n",
        "      all_labels = []\n",
        "      all_probabilities = []\n",
        "\n",
        "      for inputs, labels in train_dataloader:  # Assuming you have a DataLoader named train_loader\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs = model(inputs).squeeze(1)  # get the model's predictions\n",
        "          loss = criterion(outputs, labels.float())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          # Converting outputs to probabilities\n",
        "          probabilities = torch.sigmoid(outputs)\n",
        "          predictions = (probabilities > 0.5).float()\n",
        "          all_predictions.extend(predictions.cpu().numpy())\n",
        "          all_probabilities.extend(probabilities.cpu().detach().numpy())\n",
        "          all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "      epoch_loss = running_loss / len(train_dataloader)\n",
        "      train_f1 = f1_score(all_labels, all_predictions)\n",
        "      train_acc = accuracy_score(all_labels, all_predictions)\n",
        "      precision = precision_score(all_labels, all_predictions)\n",
        "      recall = recall_score(all_labels, all_predictions)\n",
        "      #train_auc = roc_auc_score(all_labels, all_probabilities)\n",
        "\n",
        "      model.eval()  # Set the model to evaluation mode\n",
        "      test_loss = 0.0\n",
        "      test_predictions = []\n",
        "      test_labels = []\n",
        "      test_probabilities = []\n",
        "\n",
        "      with torch.no_grad():  # Disable gradient calculation\n",
        "          for inputs, labels in test_dataloader:  # Assuming you have a DataLoader named test_dataloader\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "              outputs = model(inputs).squeeze(1)\n",
        "              loss = criterion(outputs, labels.float())\n",
        "              test_loss += loss.item()\n",
        "\n",
        "              probabilities = torch.sigmoid(outputs)\n",
        "              predictions = (probabilities > 0.5).float()\n",
        "              test_predictions.extend(predictions.cpu().numpy())\n",
        "              test_probabilities.extend(probabilities.cpu().numpy())\n",
        "              test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "      test_loss /= len(test_dataloader)\n",
        "      test_f1 = f1_score(test_labels, test_predictions)\n",
        "      test_acc = accuracy_score(test_labels, test_predictions)\n",
        "      test_precision = precision_score(test_labels, test_predictions)\n",
        "      test_recall = recall_score(test_labels, test_predictions)\n",
        "      #test_auc = roc_auc_score(test_labels, test_probabilities)\n",
        "\n",
        "      print(f\"Epoch {epoch + 1}, train_f1: {train_f1}, train Accuracy: {train_acc}, test_f1: {test_f1}, test Accuracy: {test_acc}\")\n",
        "\n",
        "      # Early stopping logic\n",
        "      if test_f1 > best_test_f1:\n",
        "        best_test_f1 = test_f1\n",
        "        epochs_without_improvement = 0\n",
        "      else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "      if epochs_without_improvement >= patience:\n",
        "        print(f\"Early stopping due to no improvement in test F1 score and best f1 observed at epoch {epoch - patience -1}\")\n",
        "        break\n",
        "\n",
        "      return best_test_f1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i9ZNGh4C2va5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_training_loop(model, optimizer, train_dataloader, test_dataloader, criterion)"
      ],
      "metadata": {
        "id": "Z9Li8rAsn7gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
        "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
        "  hidden_dim = trial.suggest_int(\"hidden_dim\", 128, 512)  # You can adjust the range based on your needs\n",
        "  num_layers = trial.suggest_int(\"num_layers\", 1, 3)  # You can adjust the range based on your needs\n",
        "  model = BiLSTM_Attention(input_dim=400, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=1, dropout_prob=dropout_prob).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  print(lr, dropout_prob)\n",
        "  #threshold = trial.suggest_uniform(\"threshold\", 0.5, 1)\n",
        "  best_test_f1 = model_training_loop(model, optimizer, train_dataloader, test_dataloader, criterion)\n",
        "  return best_test_f1\n"
      ],
      "metadata": {
        "id": "13QRt4Mn2FXk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jc46Sfw6DHU",
        "outputId": "cf026c16-1612-4a16-b47e-2e0099d5185d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:20:55,728] A new study created in memory with name: no-name-7d014598-8b7d-4de0-b322-ffa6d066819d\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08000434414323418 0.09681175202299197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:21:07,195] Trial 0 finished with value: 0.0 and parameters: {'lr': 0.08000434414323418, 'dropout_prob': 0.09681175202299197, 'hidden_dim': 332, 'num_layers': 2}. Best is trial 0 with value: 0.0.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5031712473572938, train Accuracy: 0.5074665968037726, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "7.31154157828076e-05 0.40542829408679304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:21:19,605] Trial 1 finished with value: 0.0 and parameters: {'lr': 7.31154157828076e-05, 'dropout_prob': 0.40542829408679304, 'hidden_dim': 376, 'num_layers': 3}. Best is trial 0 with value: 0.0.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.11195227339898928 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4103179364127174, train Accuracy: 0.48493581346607284, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0012668464587653205 0.11195227339898928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:21:29,406] Trial 2 finished with value: 0.0 and parameters: {'lr': 0.0012668464587653205, 'dropout_prob': 0.11195227339898928, 'hidden_dim': 230, 'num_layers': 1}. Best is trial 0 with value: 0.0.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5676975945017182, train Accuracy: 0.5056326958344249, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "2.5228496862266835e-05 0.23634207119001177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:21:40,796] Trial 3 finished with value: 0.0 and parameters: {'lr': 2.5228496862266835e-05, 'dropout_prob': 0.23634207119001177, 'hidden_dim': 257, 'num_layers': 2}. Best is trial 0 with value: 0.0.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2706106334095235 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5236714975845411, train Accuracy: 0.4833638983494891, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0007039219331813806 0.2706106334095235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:21:50,158] Trial 4 finished with value: 0.04046242774566475 and parameters: {'lr': 0.0007039219331813806, 'dropout_prob': 0.2706106334095235, 'hidden_dim': 180, 'num_layers': 1}. Best is trial 4 with value: 0.04046242774566475.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.37971391417425226, train Accuracy: 0.500130992926382, test_f1: 0.04046242774566475, test Accuracy: 0.576530612244898\n",
            "0.06903266685485074 0.03825860267719544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:22:02,511] Trial 5 finished with value: 0.0 and parameters: {'lr': 0.06903266685485074, 'dropout_prob': 0.03825860267719544, 'hidden_dim': 344, 'num_layers': 3}. Best is trial 4 with value: 0.04046242774566475.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.16581124509912015 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5027465341354957, train Accuracy: 0.5019648938957296, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0008647267208689863 0.16581124509912015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:22:12,328] Trial 6 finished with value: 0.5680365296803653 and parameters: {'lr': 0.0008647267208689863, 'dropout_prob': 0.16581124509912015, 'hidden_dim': 361, 'num_layers': 1}. Best is trial 6 with value: 0.5680365296803653.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4750684931506849, train Accuracy: 0.4980351061042704, test_f1: 0.5680365296803653, test Accuracy: 0.39668367346938777\n",
            "6.332942044312139e-05 0.4076430727018542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:22:25,400] Trial 7 finished with value: 0.0 and parameters: {'lr': 6.332942044312139e-05, 'dropout_prob': 0.4076430727018542, 'hidden_dim': 402, 'num_layers': 3}. Best is trial 6 with value: 0.5680365296803653.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4485442490631306, train Accuracy: 0.49882106366256224, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0002314507935229508 0.22240499923736956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:22:38,719] Trial 8 finished with value: 0.0 and parameters: {'lr': 0.0002314507935229508, 'dropout_prob': 0.22240499923736956, 'hidden_dim': 484, 'num_layers': 3}. Best is trial 6 with value: 0.5680365296803653.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5506035283194057, train Accuracy: 0.49279538904899134, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.006874763547924161 0.00994016478466464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:22:50,294] Trial 9 finished with value: 0.0 and parameters: {'lr': 0.006874763547924161, 'dropout_prob': 0.00994016478466464, 'hidden_dim': 390, 'num_layers': 2}. Best is trial 6 with value: 0.5680365296803653.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.16295558987790598 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4823045267489712, train Accuracy: 0.5056326958344249, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "1.4266244661489338e-05 0.16295558987790598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:23:00,222] Trial 10 finished with value: 0.0 and parameters: {'lr': 1.4266244661489338e-05, 'dropout_prob': 0.16295558987790598, 'hidden_dim': 474, 'num_layers': 1}. Best is trial 6 with value: 0.5680365296803653.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30911161878152055 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.0, train Accuracy: 0.5019648938957296, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0007373066688559366 0.30911161878152055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:23:09,413] Trial 11 finished with value: 0.5789473684210527 and parameters: {'lr': 0.0007373066688559366, 'dropout_prob': 0.30911161878152055, 'hidden_dim': 129, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.45433919455473615, train Accuracy: 0.4959392192821588, test_f1: 0.5789473684210527, test Accuracy: 0.42857142857142855\n",
            "0.00191111990977718 0.31293086707284146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.31293086707284146 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "[I 2023-10-01 11:23:18,526] Trial 12 finished with value: 0.5680365296803653 and parameters: {'lr': 0.00191111990977718, 'dropout_prob': 0.31293086707284146, 'hidden_dim': 132, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.434052757793765, train Accuracy: 0.505370709981661, test_f1: 0.5680365296803653, test Accuracy: 0.39668367346938777\n",
            "0.00033161618368222163 0.3346413516624108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3346413516624108 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:23:28,431] Trial 13 finished with value: 0.0 and parameters: {'lr': 0.00033161618368222163, 'dropout_prob': 0.3346413516624108, 'hidden_dim': 273, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4910675055516974 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5099722292350416, train Accuracy: 0.4914854597851716, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.004304188799156376 0.4910675055516974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:23:38,417] Trial 14 finished with value: 0.5502762430939226 and parameters: {'lr': 0.004304188799156376, 'dropout_prob': 0.4910675055516974, 'hidden_dim': 426, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.37557755775577556, train Accuracy: 0.5043227665706052, test_f1: 0.5502762430939226, test Accuracy: 0.48086734693877553\n",
            "0.000257896200671677 0.17903963856843413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:23:49,466] Trial 15 finished with value: 0.0 and parameters: {'lr': 0.000257896200671677, 'dropout_prob': 0.17903963856843413, 'hidden_dim': 292, 'num_layers': 2}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.20231124718441834 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4467713787085515, train Accuracy: 0.5017029080429657, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.005909289409427996 0.20231124718441834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:23:59,246] Trial 16 finished with value: 0.0 and parameters: {'lr': 0.005909289409427996, 'dropout_prob': 0.20231124718441834, 'hidden_dim': 212, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.44165694282380397, train Accuracy: 0.49855907780979825, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.000795187278696739 0.26160003178869257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:24:09,939] Trial 17 finished with value: 0.0 and parameters: {'lr': 0.000795187278696739, 'dropout_prob': 0.26160003178869257, 'hidden_dim': 136, 'num_layers': 2}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.14855283940959804 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.47151812482965383, train Accuracy: 0.4920094314906995, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.00010891685966208433 0.14855283940959804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:24:20,004] Trial 18 finished with value: 0.0 and parameters: {'lr': 0.00010891685966208433, 'dropout_prob': 0.14855283940959804, 'hidden_dim': 446, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1011824143195996 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5655737704918032, train Accuracy: 0.500130992926382, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.002756343040254434 0.1011824143195996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:24:29,901] Trial 19 finished with value: 0.0 and parameters: {'lr': 0.002756343040254434, 'dropout_prob': 0.1011824143195996, 'hidden_dim': 306, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5827050997782705, train Accuracy: 0.5069426250982447, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0005726019289697086 0.3152465072363465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:24:41,486] Trial 20 finished with value: 0.0 and parameters: {'lr': 0.0005726019289697086, 'dropout_prob': 0.3152465072363465, 'hidden_dim': 357, 'num_layers': 2}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30242897311998407 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.45065601825442103, train Accuracy: 0.49541524757663086, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.001700595077842467 0.30242897311998407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:24:50,733] Trial 21 finished with value: 0.5680365296803653 and parameters: {'lr': 0.001700595077842467, 'dropout_prob': 0.30242897311998407, 'hidden_dim': 129, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5279799247176914, train Accuracy: 0.5072046109510087, test_f1: 0.5680365296803653, test Accuracy: 0.39668367346938777\n",
            "0.0021576089801188594 0.2199191216786221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2199191216786221 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "[I 2023-10-01 11:25:00,259] Trial 22 finished with value: 0.5753931544865865 and parameters: {'lr': 0.0021576089801188594, 'dropout_prob': 0.2199191216786221, 'hidden_dim': 178, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5061287027579162, train Accuracy: 0.49331936075451926, test_f1: 0.5753931544865865, test Accuracy: 0.4145408163265306\n",
            "0.01137458717168789 0.2146284174197451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2146284174197451 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "[I 2023-10-01 11:25:09,743] Trial 23 finished with value: 0.5685557586837293 and parameters: {'lr': 0.01137458717168789, 'dropout_prob': 0.2146284174197451, 'hidden_dim': 181, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.40409683426443205, train Accuracy: 0.4969871626932146, test_f1: 0.5685557586837293, test Accuracy: 0.3979591836734694\n",
            "0.01374725282123881 0.2186525484198238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2186525484198238 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "[I 2023-10-01 11:25:19,192] Trial 24 finished with value: 0.5667276051188299 and parameters: {'lr': 0.01374725282123881, 'dropout_prob': 0.2186525484198238, 'hidden_dim': 174, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.510294500912171, train Accuracy: 0.5077285826565365, test_f1: 0.5667276051188299, test Accuracy: 0.39540816326530615\n",
            "0.010870070017120065 0.2691686379337032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2691686379337032 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "[I 2023-10-01 11:25:28,548] Trial 25 finished with value: 0.4025974025974026 and parameters: {'lr': 0.010870070017120065, 'dropout_prob': 0.2691686379337032, 'hidden_dim': 176, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5159629248197735, train Accuracy: 0.5074665968037726, test_f1: 0.4025974025974026, test Accuracy: 0.5306122448979592\n",
            "0.0030561231037985496 0.2425627833089603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:25:39,806] Trial 26 finished with value: 0.0 and parameters: {'lr': 0.0030561231037985496, 'dropout_prob': 0.2425627833089603, 'hidden_dim': 206, 'num_layers': 2}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.19603361534303226 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4, train Accuracy: 0.49541524757663086, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.024964571869314386 0.19603361534303226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:25:49,572] Trial 27 finished with value: 0.5368663594470046 and parameters: {'lr': 0.024964571869314386, 'dropout_prob': 0.19603361534303226, 'hidden_dim': 238, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4944119212346993, train Accuracy: 0.5022268797484936, test_f1: 0.5368663594470046, test Accuracy: 0.4872448979591837\n",
            "0.0019577158771346905 0.3551402909098276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3551402909098276 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:25:59,030] Trial 28 finished with value: 0.0 and parameters: {'lr': 0.0019577158771346905, 'dropout_prob': 0.3551402909098276, 'hidden_dim': 161, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5110590684361176, train Accuracy: 0.5077285826565365, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.005357162681874256 0.1293451138609766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:26:10,122] Trial 29 finished with value: 0.0 and parameters: {'lr': 0.005357162681874256, 'dropout_prob': 0.1293451138609766, 'hidden_dim': 212, 'num_layers': 2}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.529848957084632, train Accuracy: 0.48624574272989257, test_f1: 0.0, test Accuracy: 0.6007653061224489\n",
            "0.027764832237900605 0.18950142023562971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:26:21,453] Trial 30 finished with value: 0.0 and parameters: {'lr': 0.027764832237900605, 'dropout_prob': 0.18950142023562971, 'hidden_dim': 202, 'num_layers': 2}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.15220631155443942 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4812433011789925, train Accuracy: 0.49279538904899134, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0009603663534534118 0.15220631155443942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:26:31,339] Trial 31 finished with value: 0.5680365296803653 and parameters: {'lr': 0.0009603663534534118, 'dropout_prob': 0.15220631155443942, 'hidden_dim': 322, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5107033639143731, train Accuracy: 0.4969871626932146, test_f1: 0.5680365296803653, test Accuracy: 0.39668367346938777\n",
            "0.001334246376625509 0.17431156852067214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.17431156852067214 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:26:40,471] Trial 32 finished with value: 0.0 and parameters: {'lr': 0.001334246376625509, 'dropout_prob': 0.17431156852067214, 'hidden_dim': 152, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.07194154446528526 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4749034749034749, train Accuracy: 0.5011789363374378, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0004435318494801931 0.07194154446528526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:26:50,170] Trial 33 finished with value: 0.49830890642615555 and parameters: {'lr': 0.0004435318494801931, 'dropout_prob': 0.07194154446528526, 'hidden_dim': 256, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5166749626679941, train Accuracy: 0.4912234739324077, test_f1: 0.49830890642615555, test Accuracy: 0.43239795918367346\n",
            "0.0010494007747558875 0.13396214810867424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.13396214810867424 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "[I 2023-10-01 11:26:59,995] Trial 34 finished with value: 0.5680365296803653 and parameters: {'lr': 0.0010494007747558875, 'dropout_prob': 0.13396214810867424, 'hidden_dim': 193, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4238329238329238, train Accuracy: 0.5085145402148284, test_f1: 0.5680365296803653, test Accuracy: 0.39668367346938777\n",
            "0.0028570304625622884 0.22842023360923225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.22842023360923225 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:27:09,242] Trial 35 finished with value: 0.0 and parameters: {'lr': 0.0028570304625622884, 'dropout_prob': 0.22842023360923225, 'hidden_dim': 155, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25378235818224687 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4384707287933094, train Accuracy: 0.5074665968037726, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0005581792228162584 0.25378235818224687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:27:19,278] Trial 36 finished with value: 0.2857142857142857 and parameters: {'lr': 0.0005581792228162584, 'dropout_prob': 0.25378235818224687, 'hidden_dim': 511, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.3400267737617135, train Accuracy: 0.4833638983494891, test_f1: 0.2857142857142857, test Accuracy: 0.4897959183673469\n",
            "0.0011297689518307184 0.2798106663093412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2798106663093412 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:27:29,173] Trial 37 finished with value: 0.0 and parameters: {'lr': 0.0011297689518307184, 'dropout_prob': 0.2798106663093412, 'hidden_dim': 341, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.20604818482902443 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4018264840182649, train Accuracy: 0.4851977993188368, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.003536326521028434 0.20604818482902443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:27:38,985] Trial 38 finished with value: 0.04387568555758684 and parameters: {'lr': 0.003536326521028434, 'dropout_prob': 0.20604818482902443, 'hidden_dim': 234, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5270648030495554, train Accuracy: 0.5124443280062877, test_f1: 0.04387568555758684, test Accuracy: 0.33290816326530615\n",
            "0.001684132029889593 0.2452338580960347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:27:52,268] Trial 39 finished with value: 0.0 and parameters: {'lr': 0.001684132029889593, 'dropout_prob': 0.2452338580960347, 'hidden_dim': 381, 'num_layers': 3}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.453257790368272, train Accuracy: 0.49436730416557506, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0007386050414951656 0.07990498152245143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:28:03,774] Trial 40 finished with value: 0.5680365296803653 and parameters: {'lr': 0.0007386050414951656, 'dropout_prob': 0.07990498152245143, 'hidden_dim': 279, 'num_layers': 2}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4654983570646221, train Accuracy: 0.48860361540476815, test_f1: 0.5680365296803653, test Accuracy: 0.39668367346938777\n",
            "0.001982266586766223 0.2912010960712311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2912010960712311 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:28:12,912] Trial 41 finished with value: 0.0 and parameters: {'lr': 0.001982266586766223, 'dropout_prob': 0.2912010960712311, 'hidden_dim': 144, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.22780984757659067 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.3886225631192074, train Accuracy: 0.49882106366256224, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0003504183065657813 0.22780984757659067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:28:22,397] Trial 42 finished with value: 0.0 and parameters: {'lr': 0.0003504183065657813, 'dropout_prob': 0.22780984757659067, 'hidden_dim': 171, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.28095200978760343 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.47400109469074986, train Accuracy: 0.49646319098768665, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0021192090333262427 0.28095200978760343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:28:31,630] Trial 43 finished with value: 0.0 and parameters: {'lr': 0.0021192090333262427, 'dropout_prob': 0.28095200978760343, 'hidden_dim': 129, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.17431639239549548 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5662289794978115, train Accuracy: 0.5066806392454808, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0012172675349808773 0.17431639239549548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-01 11:28:41,231] Trial 44 finished with value: 0.4005681818181819 and parameters: {'lr': 0.0012172675349808773, 'dropout_prob': 0.17431639239549548, 'hidden_dim': 186, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.47168216398985624, train Accuracy: 0.5087765260675924, test_f1: 0.4005681818181819, test Accuracy: 0.461734693877551\n",
            "0.004788151973598644 0.328827312754402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.328827312754402 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "[I 2023-10-01 11:28:50,508] Trial 45 finished with value: 0.5680365296803653 and parameters: {'lr': 0.004788151973598644, 'dropout_prob': 0.328827312754402, 'hidden_dim': 157, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.3113859585303747, train Accuracy: 0.5040607807178412, test_f1: 0.5680365296803653, test Accuracy: 0.39668367346938777\n",
            "0.00019608488926972685 0.35491668140503996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.35491668140503996 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "[I 2023-10-01 11:29:00,416] Trial 46 finished with value: 0.5680365296803653 and parameters: {'lr': 0.00019608488926972685, 'dropout_prob': 0.35491668140503996, 'hidden_dim': 357, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.47477253928866836, train Accuracy: 0.5009169504846738, test_f1: 0.5680365296803653, test Accuracy: 0.39668367346938777\n",
            "0.0007106028879978587 0.20997517837439048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.20997517837439048 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "[I 2023-10-01 11:29:10,325] Trial 47 finished with value: 0.5685557586837293 and parameters: {'lr': 0.0007106028879978587, 'dropout_prob': 0.20997517837439048, 'hidden_dim': 399, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5415266442630821, train Accuracy: 0.4996070212208541, test_f1: 0.5685557586837293, test Accuracy: 0.3979591836734694\n",
            "0.0006965983179405797 0.2137081166285304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2137081166285304 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:29:20,197] Trial 48 finished with value: 0.0 and parameters: {'lr': 0.0006965983179405797, 'dropout_prob': 0.2137081166285304, 'hidden_dim': 409, 'num_layers': 1}. Best is trial 11 with value: 0.5789473684210527.\n",
            "<ipython-input-28-d02b52dcc8b9>:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-28-d02b52dcc8b9>:3: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_prob = trial.suggest_uniform(\"dropout_prob\", 0, 0.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.4830230010952903, train Accuracy: 0.505370709981661, test_f1: 0.0, test Accuracy: 0.6033163265306123\n",
            "0.0005239091909184562 0.19005325187514965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[I 2023-10-01 11:29:33,574] Trial 49 finished with value: 0.0 and parameters: {'lr': 0.0005239091909184562, 'dropout_prob': 0.19005325187514965, 'hidden_dim': 433, 'num_layers': 3}. Best is trial 11 with value: 0.5789473684210527.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train_f1: 0.5080687142113482, train Accuracy: 0.5048467382761331, test_f1: 0.0, test Accuracy: 0.6033163265306123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completed_trials = study.trials_dataframe()"
      ],
      "metadata": {
        "id": "ibVm7k_C-pPY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completed_trials"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6JEw8BF6AXIQ",
        "outputId": "058928d4-b7e6-4c98-9ba0-bd9cebc4a130"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    number     value             datetime_start          datetime_complete  \\\n",
              "0        0  0.000000 2023-10-01 11:20:55.731875 2023-10-01 11:21:07.195325   \n",
              "1        1  0.000000 2023-10-01 11:21:07.196488 2023-10-01 11:21:19.605359   \n",
              "2        2  0.000000 2023-10-01 11:21:19.606448 2023-10-01 11:21:29.406349   \n",
              "3        3  0.000000 2023-10-01 11:21:29.407529 2023-10-01 11:21:40.796614   \n",
              "4        4  0.040462 2023-10-01 11:21:40.797748 2023-10-01 11:21:50.157985   \n",
              "5        5  0.000000 2023-10-01 11:21:50.159188 2023-10-01 11:22:02.510876   \n",
              "6        6  0.568037 2023-10-01 11:22:02.512804 2023-10-01 11:22:12.328257   \n",
              "7        7  0.000000 2023-10-01 11:22:12.329491 2023-10-01 11:22:25.400230   \n",
              "8        8  0.000000 2023-10-01 11:22:25.401353 2023-10-01 11:22:38.718939   \n",
              "9        9  0.000000 2023-10-01 11:22:38.720190 2023-10-01 11:22:50.294501   \n",
              "10      10  0.000000 2023-10-01 11:22:50.295680 2023-10-01 11:23:00.222220   \n",
              "11      11  0.578947 2023-10-01 11:23:00.223450 2023-10-01 11:23:09.412948   \n",
              "12      12  0.568037 2023-10-01 11:23:09.414193 2023-10-01 11:23:18.525841   \n",
              "13      13  0.000000 2023-10-01 11:23:18.527695 2023-10-01 11:23:28.430803   \n",
              "14      14  0.550276 2023-10-01 11:23:28.431974 2023-10-01 11:23:38.417375   \n",
              "15      15  0.000000 2023-10-01 11:23:38.418532 2023-10-01 11:23:49.465965   \n",
              "16      16  0.000000 2023-10-01 11:23:49.467104 2023-10-01 11:23:59.246229   \n",
              "17      17  0.000000 2023-10-01 11:23:59.247409 2023-10-01 11:24:09.938982   \n",
              "18      18  0.000000 2023-10-01 11:24:09.940186 2023-10-01 11:24:20.004075   \n",
              "19      19  0.000000 2023-10-01 11:24:20.005171 2023-10-01 11:24:29.901336   \n",
              "20      20  0.000000 2023-10-01 11:24:29.902441 2023-10-01 11:24:41.485812   \n",
              "21      21  0.568037 2023-10-01 11:24:41.486948 2023-10-01 11:24:50.733463   \n",
              "22      22  0.575393 2023-10-01 11:24:50.734648 2023-10-01 11:25:00.259160   \n",
              "23      23  0.568556 2023-10-01 11:25:00.260278 2023-10-01 11:25:09.742962   \n",
              "24      24  0.566728 2023-10-01 11:25:09.744112 2023-10-01 11:25:19.192372   \n",
              "25      25  0.402597 2023-10-01 11:25:19.193591 2023-10-01 11:25:28.548715   \n",
              "26      26  0.000000 2023-10-01 11:25:28.549857 2023-10-01 11:25:39.806226   \n",
              "27      27  0.536866 2023-10-01 11:25:39.807370 2023-10-01 11:25:49.572054   \n",
              "28      28  0.000000 2023-10-01 11:25:49.573539 2023-10-01 11:25:59.030235   \n",
              "29      29  0.000000 2023-10-01 11:25:59.031272 2023-10-01 11:26:10.122049   \n",
              "30      30  0.000000 2023-10-01 11:26:10.123384 2023-10-01 11:26:21.453508   \n",
              "31      31  0.568037 2023-10-01 11:26:21.454620 2023-10-01 11:26:31.339339   \n",
              "32      32  0.000000 2023-10-01 11:26:31.340623 2023-10-01 11:26:40.470992   \n",
              "33      33  0.498309 2023-10-01 11:26:40.472081 2023-10-01 11:26:50.170163   \n",
              "34      34  0.568037 2023-10-01 11:26:50.171318 2023-10-01 11:26:59.995497   \n",
              "35      35  0.000000 2023-10-01 11:26:59.996668 2023-10-01 11:27:09.242695   \n",
              "36      36  0.285714 2023-10-01 11:27:09.243822 2023-10-01 11:27:19.278508   \n",
              "37      37  0.000000 2023-10-01 11:27:19.279705 2023-10-01 11:27:29.173078   \n",
              "38      38  0.043876 2023-10-01 11:27:29.174595 2023-10-01 11:27:38.985024   \n",
              "39      39  0.000000 2023-10-01 11:27:38.986187 2023-10-01 11:27:52.268025   \n",
              "40      40  0.568037 2023-10-01 11:27:52.269164 2023-10-01 11:28:03.774637   \n",
              "41      41  0.000000 2023-10-01 11:28:03.775871 2023-10-01 11:28:12.912597   \n",
              "42      42  0.000000 2023-10-01 11:28:12.913786 2023-10-01 11:28:22.397378   \n",
              "43      43  0.000000 2023-10-01 11:28:22.398513 2023-10-01 11:28:31.629972   \n",
              "44      44  0.400568 2023-10-01 11:28:31.631115 2023-10-01 11:28:41.231671   \n",
              "45      45  0.568037 2023-10-01 11:28:41.232848 2023-10-01 11:28:50.507932   \n",
              "46      46  0.568037 2023-10-01 11:28:50.509145 2023-10-01 11:29:00.416039   \n",
              "47      47  0.568556 2023-10-01 11:29:00.417296 2023-10-01 11:29:10.324848   \n",
              "48      48  0.000000 2023-10-01 11:29:10.326062 2023-10-01 11:29:20.197058   \n",
              "49      49  0.000000 2023-10-01 11:29:20.198177 2023-10-01 11:29:33.574290   \n",
              "\n",
              "                 duration  params_dropout_prob  params_hidden_dim  params_lr  \\\n",
              "0  0 days 00:00:11.463450             0.096812                332   0.080004   \n",
              "1  0 days 00:00:12.408871             0.405428                376   0.000073   \n",
              "2  0 days 00:00:09.799901             0.111952                230   0.001267   \n",
              "3  0 days 00:00:11.389085             0.236342                257   0.000025   \n",
              "4  0 days 00:00:09.360237             0.270611                180   0.000704   \n",
              "5  0 days 00:00:12.351688             0.038259                344   0.069033   \n",
              "6  0 days 00:00:09.815453             0.165811                361   0.000865   \n",
              "7  0 days 00:00:13.070739             0.407643                402   0.000063   \n",
              "8  0 days 00:00:13.317586             0.222405                484   0.000231   \n",
              "9  0 days 00:00:11.574311             0.009940                390   0.006875   \n",
              "10 0 days 00:00:09.926540             0.162956                474   0.000014   \n",
              "11 0 days 00:00:09.189498             0.309112                129   0.000737   \n",
              "12 0 days 00:00:09.111648             0.312931                132   0.001911   \n",
              "13 0 days 00:00:09.903108             0.334641                273   0.000332   \n",
              "14 0 days 00:00:09.985401             0.491068                426   0.004304   \n",
              "15 0 days 00:00:11.047433             0.179040                292   0.000258   \n",
              "16 0 days 00:00:09.779125             0.202311                212   0.005909   \n",
              "17 0 days 00:00:10.691573             0.261600                136   0.000795   \n",
              "18 0 days 00:00:10.063889             0.148553                446   0.000109   \n",
              "19 0 days 00:00:09.896165             0.101182                306   0.002756   \n",
              "20 0 days 00:00:11.583371             0.315247                357   0.000573   \n",
              "21 0 days 00:00:09.246515             0.302429                129   0.001701   \n",
              "22 0 days 00:00:09.524512             0.219919                178   0.002158   \n",
              "23 0 days 00:00:09.482684             0.214628                181   0.011375   \n",
              "24 0 days 00:00:09.448260             0.218653                174   0.013747   \n",
              "25 0 days 00:00:09.355124             0.269169                176   0.010870   \n",
              "26 0 days 00:00:11.256369             0.242563                206   0.003056   \n",
              "27 0 days 00:00:09.764684             0.196034                238   0.024965   \n",
              "28 0 days 00:00:09.456696             0.355140                161   0.001958   \n",
              "29 0 days 00:00:11.090777             0.129345                212   0.005357   \n",
              "30 0 days 00:00:11.330124             0.189501                202   0.027765   \n",
              "31 0 days 00:00:09.884719             0.152206                322   0.000960   \n",
              "32 0 days 00:00:09.130369             0.174312                152   0.001334   \n",
              "33 0 days 00:00:09.698082             0.071942                256   0.000444   \n",
              "34 0 days 00:00:09.824179             0.133962                193   0.001049   \n",
              "35 0 days 00:00:09.246027             0.228420                155   0.002857   \n",
              "36 0 days 00:00:10.034686             0.253782                511   0.000558   \n",
              "37 0 days 00:00:09.893373             0.279811                341   0.001130   \n",
              "38 0 days 00:00:09.810429             0.206048                234   0.003536   \n",
              "39 0 days 00:00:13.281838             0.245234                381   0.001684   \n",
              "40 0 days 00:00:11.505473             0.079905                279   0.000739   \n",
              "41 0 days 00:00:09.136726             0.291201                144   0.001982   \n",
              "42 0 days 00:00:09.483592             0.227810                171   0.000350   \n",
              "43 0 days 00:00:09.231459             0.280952                129   0.002119   \n",
              "44 0 days 00:00:09.600556             0.174316                186   0.001217   \n",
              "45 0 days 00:00:09.275084             0.328827                157   0.004788   \n",
              "46 0 days 00:00:09.906894             0.354917                357   0.000196   \n",
              "47 0 days 00:00:09.907552             0.209975                399   0.000711   \n",
              "48 0 days 00:00:09.870996             0.213708                409   0.000697   \n",
              "49 0 days 00:00:13.376113             0.190053                433   0.000524   \n",
              "\n",
              "    params_num_layers     state  \n",
              "0                   2  COMPLETE  \n",
              "1                   3  COMPLETE  \n",
              "2                   1  COMPLETE  \n",
              "3                   2  COMPLETE  \n",
              "4                   1  COMPLETE  \n",
              "5                   3  COMPLETE  \n",
              "6                   1  COMPLETE  \n",
              "7                   3  COMPLETE  \n",
              "8                   3  COMPLETE  \n",
              "9                   2  COMPLETE  \n",
              "10                  1  COMPLETE  \n",
              "11                  1  COMPLETE  \n",
              "12                  1  COMPLETE  \n",
              "13                  1  COMPLETE  \n",
              "14                  1  COMPLETE  \n",
              "15                  2  COMPLETE  \n",
              "16                  1  COMPLETE  \n",
              "17                  2  COMPLETE  \n",
              "18                  1  COMPLETE  \n",
              "19                  1  COMPLETE  \n",
              "20                  2  COMPLETE  \n",
              "21                  1  COMPLETE  \n",
              "22                  1  COMPLETE  \n",
              "23                  1  COMPLETE  \n",
              "24                  1  COMPLETE  \n",
              "25                  1  COMPLETE  \n",
              "26                  2  COMPLETE  \n",
              "27                  1  COMPLETE  \n",
              "28                  1  COMPLETE  \n",
              "29                  2  COMPLETE  \n",
              "30                  2  COMPLETE  \n",
              "31                  1  COMPLETE  \n",
              "32                  1  COMPLETE  \n",
              "33                  1  COMPLETE  \n",
              "34                  1  COMPLETE  \n",
              "35                  1  COMPLETE  \n",
              "36                  1  COMPLETE  \n",
              "37                  1  COMPLETE  \n",
              "38                  1  COMPLETE  \n",
              "39                  3  COMPLETE  \n",
              "40                  2  COMPLETE  \n",
              "41                  1  COMPLETE  \n",
              "42                  1  COMPLETE  \n",
              "43                  1  COMPLETE  \n",
              "44                  1  COMPLETE  \n",
              "45                  1  COMPLETE  \n",
              "46                  1  COMPLETE  \n",
              "47                  1  COMPLETE  \n",
              "48                  1  COMPLETE  \n",
              "49                  3  COMPLETE  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d3d902c-c865-449d-9324-a1bb0f3ffd20\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>number</th>\n",
              "      <th>value</th>\n",
              "      <th>datetime_start</th>\n",
              "      <th>datetime_complete</th>\n",
              "      <th>duration</th>\n",
              "      <th>params_dropout_prob</th>\n",
              "      <th>params_hidden_dim</th>\n",
              "      <th>params_lr</th>\n",
              "      <th>params_num_layers</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:20:55.731875</td>\n",
              "      <td>2023-10-01 11:21:07.195325</td>\n",
              "      <td>0 days 00:00:11.463450</td>\n",
              "      <td>0.096812</td>\n",
              "      <td>332</td>\n",
              "      <td>0.080004</td>\n",
              "      <td>2</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:21:07.196488</td>\n",
              "      <td>2023-10-01 11:21:19.605359</td>\n",
              "      <td>0 days 00:00:12.408871</td>\n",
              "      <td>0.405428</td>\n",
              "      <td>376</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>3</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:21:19.606448</td>\n",
              "      <td>2023-10-01 11:21:29.406349</td>\n",
              "      <td>0 days 00:00:09.799901</td>\n",
              "      <td>0.111952</td>\n",
              "      <td>230</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:21:29.407529</td>\n",
              "      <td>2023-10-01 11:21:40.796614</td>\n",
              "      <td>0 days 00:00:11.389085</td>\n",
              "      <td>0.236342</td>\n",
              "      <td>257</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>2</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.040462</td>\n",
              "      <td>2023-10-01 11:21:40.797748</td>\n",
              "      <td>2023-10-01 11:21:50.157985</td>\n",
              "      <td>0 days 00:00:09.360237</td>\n",
              "      <td>0.270611</td>\n",
              "      <td>180</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:21:50.159188</td>\n",
              "      <td>2023-10-01 11:22:02.510876</td>\n",
              "      <td>0 days 00:00:12.351688</td>\n",
              "      <td>0.038259</td>\n",
              "      <td>344</td>\n",
              "      <td>0.069033</td>\n",
              "      <td>3</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.568037</td>\n",
              "      <td>2023-10-01 11:22:02.512804</td>\n",
              "      <td>2023-10-01 11:22:12.328257</td>\n",
              "      <td>0 days 00:00:09.815453</td>\n",
              "      <td>0.165811</td>\n",
              "      <td>361</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:22:12.329491</td>\n",
              "      <td>2023-10-01 11:22:25.400230</td>\n",
              "      <td>0 days 00:00:13.070739</td>\n",
              "      <td>0.407643</td>\n",
              "      <td>402</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>3</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:22:25.401353</td>\n",
              "      <td>2023-10-01 11:22:38.718939</td>\n",
              "      <td>0 days 00:00:13.317586</td>\n",
              "      <td>0.222405</td>\n",
              "      <td>484</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>3</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:22:38.720190</td>\n",
              "      <td>2023-10-01 11:22:50.294501</td>\n",
              "      <td>0 days 00:00:11.574311</td>\n",
              "      <td>0.009940</td>\n",
              "      <td>390</td>\n",
              "      <td>0.006875</td>\n",
              "      <td>2</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:22:50.295680</td>\n",
              "      <td>2023-10-01 11:23:00.222220</td>\n",
              "      <td>0 days 00:00:09.926540</td>\n",
              "      <td>0.162956</td>\n",
              "      <td>474</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>2023-10-01 11:23:00.223450</td>\n",
              "      <td>2023-10-01 11:23:09.412948</td>\n",
              "      <td>0 days 00:00:09.189498</td>\n",
              "      <td>0.309112</td>\n",
              "      <td>129</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>0.568037</td>\n",
              "      <td>2023-10-01 11:23:09.414193</td>\n",
              "      <td>2023-10-01 11:23:18.525841</td>\n",
              "      <td>0 days 00:00:09.111648</td>\n",
              "      <td>0.312931</td>\n",
              "      <td>132</td>\n",
              "      <td>0.001911</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:23:18.527695</td>\n",
              "      <td>2023-10-01 11:23:28.430803</td>\n",
              "      <td>0 days 00:00:09.903108</td>\n",
              "      <td>0.334641</td>\n",
              "      <td>273</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>0.550276</td>\n",
              "      <td>2023-10-01 11:23:28.431974</td>\n",
              "      <td>2023-10-01 11:23:38.417375</td>\n",
              "      <td>0 days 00:00:09.985401</td>\n",
              "      <td>0.491068</td>\n",
              "      <td>426</td>\n",
              "      <td>0.004304</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:23:38.418532</td>\n",
              "      <td>2023-10-01 11:23:49.465965</td>\n",
              "      <td>0 days 00:00:11.047433</td>\n",
              "      <td>0.179040</td>\n",
              "      <td>292</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>2</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:23:49.467104</td>\n",
              "      <td>2023-10-01 11:23:59.246229</td>\n",
              "      <td>0 days 00:00:09.779125</td>\n",
              "      <td>0.202311</td>\n",
              "      <td>212</td>\n",
              "      <td>0.005909</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:23:59.247409</td>\n",
              "      <td>2023-10-01 11:24:09.938982</td>\n",
              "      <td>0 days 00:00:10.691573</td>\n",
              "      <td>0.261600</td>\n",
              "      <td>136</td>\n",
              "      <td>0.000795</td>\n",
              "      <td>2</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:24:09.940186</td>\n",
              "      <td>2023-10-01 11:24:20.004075</td>\n",
              "      <td>0 days 00:00:10.063889</td>\n",
              "      <td>0.148553</td>\n",
              "      <td>446</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:24:20.005171</td>\n",
              "      <td>2023-10-01 11:24:29.901336</td>\n",
              "      <td>0 days 00:00:09.896165</td>\n",
              "      <td>0.101182</td>\n",
              "      <td>306</td>\n",
              "      <td>0.002756</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:24:29.902441</td>\n",
              "      <td>2023-10-01 11:24:41.485812</td>\n",
              "      <td>0 days 00:00:11.583371</td>\n",
              "      <td>0.315247</td>\n",
              "      <td>357</td>\n",
              "      <td>0.000573</td>\n",
              "      <td>2</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>0.568037</td>\n",
              "      <td>2023-10-01 11:24:41.486948</td>\n",
              "      <td>2023-10-01 11:24:50.733463</td>\n",
              "      <td>0 days 00:00:09.246515</td>\n",
              "      <td>0.302429</td>\n",
              "      <td>129</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>0.575393</td>\n",
              "      <td>2023-10-01 11:24:50.734648</td>\n",
              "      <td>2023-10-01 11:25:00.259160</td>\n",
              "      <td>0 days 00:00:09.524512</td>\n",
              "      <td>0.219919</td>\n",
              "      <td>178</td>\n",
              "      <td>0.002158</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>0.568556</td>\n",
              "      <td>2023-10-01 11:25:00.260278</td>\n",
              "      <td>2023-10-01 11:25:09.742962</td>\n",
              "      <td>0 days 00:00:09.482684</td>\n",
              "      <td>0.214628</td>\n",
              "      <td>181</td>\n",
              "      <td>0.011375</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>0.566728</td>\n",
              "      <td>2023-10-01 11:25:09.744112</td>\n",
              "      <td>2023-10-01 11:25:19.192372</td>\n",
              "      <td>0 days 00:00:09.448260</td>\n",
              "      <td>0.218653</td>\n",
              "      <td>174</td>\n",
              "      <td>0.013747</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>0.402597</td>\n",
              "      <td>2023-10-01 11:25:19.193591</td>\n",
              "      <td>2023-10-01 11:25:28.548715</td>\n",
              "      <td>0 days 00:00:09.355124</td>\n",
              "      <td>0.269169</td>\n",
              "      <td>176</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:25:28.549857</td>\n",
              "      <td>2023-10-01 11:25:39.806226</td>\n",
              "      <td>0 days 00:00:11.256369</td>\n",
              "      <td>0.242563</td>\n",
              "      <td>206</td>\n",
              "      <td>0.003056</td>\n",
              "      <td>2</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>0.536866</td>\n",
              "      <td>2023-10-01 11:25:39.807370</td>\n",
              "      <td>2023-10-01 11:25:49.572054</td>\n",
              "      <td>0 days 00:00:09.764684</td>\n",
              "      <td>0.196034</td>\n",
              "      <td>238</td>\n",
              "      <td>0.024965</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:25:49.573539</td>\n",
              "      <td>2023-10-01 11:25:59.030235</td>\n",
              "      <td>0 days 00:00:09.456696</td>\n",
              "      <td>0.355140</td>\n",
              "      <td>161</td>\n",
              "      <td>0.001958</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:25:59.031272</td>\n",
              "      <td>2023-10-01 11:26:10.122049</td>\n",
              "      <td>0 days 00:00:11.090777</td>\n",
              "      <td>0.129345</td>\n",
              "      <td>212</td>\n",
              "      <td>0.005357</td>\n",
              "      <td>2</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:26:10.123384</td>\n",
              "      <td>2023-10-01 11:26:21.453508</td>\n",
              "      <td>0 days 00:00:11.330124</td>\n",
              "      <td>0.189501</td>\n",
              "      <td>202</td>\n",
              "      <td>0.027765</td>\n",
              "      <td>2</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>31</td>\n",
              "      <td>0.568037</td>\n",
              "      <td>2023-10-01 11:26:21.454620</td>\n",
              "      <td>2023-10-01 11:26:31.339339</td>\n",
              "      <td>0 days 00:00:09.884719</td>\n",
              "      <td>0.152206</td>\n",
              "      <td>322</td>\n",
              "      <td>0.000960</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:26:31.340623</td>\n",
              "      <td>2023-10-01 11:26:40.470992</td>\n",
              "      <td>0 days 00:00:09.130369</td>\n",
              "      <td>0.174312</td>\n",
              "      <td>152</td>\n",
              "      <td>0.001334</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>33</td>\n",
              "      <td>0.498309</td>\n",
              "      <td>2023-10-01 11:26:40.472081</td>\n",
              "      <td>2023-10-01 11:26:50.170163</td>\n",
              "      <td>0 days 00:00:09.698082</td>\n",
              "      <td>0.071942</td>\n",
              "      <td>256</td>\n",
              "      <td>0.000444</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>34</td>\n",
              "      <td>0.568037</td>\n",
              "      <td>2023-10-01 11:26:50.171318</td>\n",
              "      <td>2023-10-01 11:26:59.995497</td>\n",
              "      <td>0 days 00:00:09.824179</td>\n",
              "      <td>0.133962</td>\n",
              "      <td>193</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>35</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:26:59.996668</td>\n",
              "      <td>2023-10-01 11:27:09.242695</td>\n",
              "      <td>0 days 00:00:09.246027</td>\n",
              "      <td>0.228420</td>\n",
              "      <td>155</td>\n",
              "      <td>0.002857</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>36</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>2023-10-01 11:27:09.243822</td>\n",
              "      <td>2023-10-01 11:27:19.278508</td>\n",
              "      <td>0 days 00:00:10.034686</td>\n",
              "      <td>0.253782</td>\n",
              "      <td>511</td>\n",
              "      <td>0.000558</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>37</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:27:19.279705</td>\n",
              "      <td>2023-10-01 11:27:29.173078</td>\n",
              "      <td>0 days 00:00:09.893373</td>\n",
              "      <td>0.279811</td>\n",
              "      <td>341</td>\n",
              "      <td>0.001130</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>38</td>\n",
              "      <td>0.043876</td>\n",
              "      <td>2023-10-01 11:27:29.174595</td>\n",
              "      <td>2023-10-01 11:27:38.985024</td>\n",
              "      <td>0 days 00:00:09.810429</td>\n",
              "      <td>0.206048</td>\n",
              "      <td>234</td>\n",
              "      <td>0.003536</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>39</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:27:38.986187</td>\n",
              "      <td>2023-10-01 11:27:52.268025</td>\n",
              "      <td>0 days 00:00:13.281838</td>\n",
              "      <td>0.245234</td>\n",
              "      <td>381</td>\n",
              "      <td>0.001684</td>\n",
              "      <td>3</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>40</td>\n",
              "      <td>0.568037</td>\n",
              "      <td>2023-10-01 11:27:52.269164</td>\n",
              "      <td>2023-10-01 11:28:03.774637</td>\n",
              "      <td>0 days 00:00:11.505473</td>\n",
              "      <td>0.079905</td>\n",
              "      <td>279</td>\n",
              "      <td>0.000739</td>\n",
              "      <td>2</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>41</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:28:03.775871</td>\n",
              "      <td>2023-10-01 11:28:12.912597</td>\n",
              "      <td>0 days 00:00:09.136726</td>\n",
              "      <td>0.291201</td>\n",
              "      <td>144</td>\n",
              "      <td>0.001982</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>42</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:28:12.913786</td>\n",
              "      <td>2023-10-01 11:28:22.397378</td>\n",
              "      <td>0 days 00:00:09.483592</td>\n",
              "      <td>0.227810</td>\n",
              "      <td>171</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:28:22.398513</td>\n",
              "      <td>2023-10-01 11:28:31.629972</td>\n",
              "      <td>0 days 00:00:09.231459</td>\n",
              "      <td>0.280952</td>\n",
              "      <td>129</td>\n",
              "      <td>0.002119</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>44</td>\n",
              "      <td>0.400568</td>\n",
              "      <td>2023-10-01 11:28:31.631115</td>\n",
              "      <td>2023-10-01 11:28:41.231671</td>\n",
              "      <td>0 days 00:00:09.600556</td>\n",
              "      <td>0.174316</td>\n",
              "      <td>186</td>\n",
              "      <td>0.001217</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>45</td>\n",
              "      <td>0.568037</td>\n",
              "      <td>2023-10-01 11:28:41.232848</td>\n",
              "      <td>2023-10-01 11:28:50.507932</td>\n",
              "      <td>0 days 00:00:09.275084</td>\n",
              "      <td>0.328827</td>\n",
              "      <td>157</td>\n",
              "      <td>0.004788</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>46</td>\n",
              "      <td>0.568037</td>\n",
              "      <td>2023-10-01 11:28:50.509145</td>\n",
              "      <td>2023-10-01 11:29:00.416039</td>\n",
              "      <td>0 days 00:00:09.906894</td>\n",
              "      <td>0.354917</td>\n",
              "      <td>357</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>47</td>\n",
              "      <td>0.568556</td>\n",
              "      <td>2023-10-01 11:29:00.417296</td>\n",
              "      <td>2023-10-01 11:29:10.324848</td>\n",
              "      <td>0 days 00:00:09.907552</td>\n",
              "      <td>0.209975</td>\n",
              "      <td>399</td>\n",
              "      <td>0.000711</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>48</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:29:10.326062</td>\n",
              "      <td>2023-10-01 11:29:20.197058</td>\n",
              "      <td>0 days 00:00:09.870996</td>\n",
              "      <td>0.213708</td>\n",
              "      <td>409</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>1</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>49</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2023-10-01 11:29:20.198177</td>\n",
              "      <td>2023-10-01 11:29:33.574290</td>\n",
              "      <td>0 days 00:00:13.376113</td>\n",
              "      <td>0.190053</td>\n",
              "      <td>433</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>3</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d3d902c-c865-449d-9324-a1bb0f3ffd20')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8d3d902c-c865-449d-9324-a1bb0f3ffd20 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8d3d902c-c865-449d-9324-a1bb0f3ffd20');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-aa3517a1-b5e3-44bf-88fb-895ba9a97f65\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa3517a1-b5e3-44bf-88fb-895ba9a97f65')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-aa3517a1-b5e3-44bf-88fb-895ba9a97f65 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "\n",
        "num_epochs = 13  # Example number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    for inputs, labels in train_dataloader:  # Assuming you have a DataLoader named train_loader\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs).squeeze(1)  # get the model's predictions\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Converting outputs to probabilities\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predictions = (probabilities > 0.5).float()\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_probabilities.extend(probabilities.cpu().detach().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader)\n",
        "    f1 = f1_score(all_labels, all_predictions)\n",
        "    acc = accuracy_score(all_labels, all_predictions)\n",
        "    precision = precision_score(all_labels, all_predictions)\n",
        "    recall = recall_score(all_labels, all_predictions)\n",
        "    auc = roc_auc_score(all_labels, all_probabilities)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Loss: {epoch_loss:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print('-' * 30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET8EJqhA1VWQ",
        "outputId": "4e31565a-0889-4ed9-9d10-d2cef6e9aaf9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/13\n",
            "Loss: 0.6945\n",
            "F1 Score: 0.4952\n",
            "Accuracy: 0.5038\n",
            "Precision: 0.5019\n",
            "Recall: 0.4887\n",
            "AUC: 0.5007\n",
            "------------------------------\n",
            "Epoch 2/13\n",
            "Loss: 0.6917\n",
            "F1 Score: 0.5575\n",
            "Accuracy: 0.5067\n",
            "Precision: 0.5038\n",
            "Recall: 0.6239\n",
            "AUC: 0.5160\n",
            "------------------------------\n",
            "Epoch 3/13\n",
            "Loss: 0.6810\n",
            "F1 Score: 0.6104\n",
            "Accuracy: 0.5476\n",
            "Precision: 0.5344\n",
            "Recall: 0.7117\n",
            "AUC: 0.5595\n",
            "------------------------------\n",
            "Epoch 4/13\n",
            "Loss: 0.6839\n",
            "F1 Score: 0.5652\n",
            "Accuracy: 0.5428\n",
            "Precision: 0.5369\n",
            "Recall: 0.5965\n",
            "AUC: 0.5657\n",
            "------------------------------\n",
            "Epoch 5/13\n",
            "Loss: 0.6650\n",
            "F1 Score: 0.6436\n",
            "Accuracy: 0.5921\n",
            "Precision: 0.5697\n",
            "Recall: 0.7396\n",
            "AUC: 0.6105\n",
            "------------------------------\n",
            "Epoch 6/13\n",
            "Loss: 0.6611\n",
            "F1 Score: 0.6491\n",
            "Accuracy: 0.5889\n",
            "Precision: 0.5646\n",
            "Recall: 0.7633\n",
            "AUC: 0.6217\n",
            "------------------------------\n",
            "Epoch 7/13\n",
            "Loss: 0.6547\n",
            "F1 Score: 0.6480\n",
            "Accuracy: 0.6041\n",
            "Precision: 0.5815\n",
            "Recall: 0.7317\n",
            "AUC: 0.6381\n",
            "------------------------------\n",
            "Epoch 8/13\n",
            "Loss: 0.6490\n",
            "F1 Score: 0.6553\n",
            "Accuracy: 0.6183\n",
            "Precision: 0.5954\n",
            "Recall: 0.7286\n",
            "AUC: 0.6544\n",
            "------------------------------\n",
            "Epoch 9/13\n",
            "Loss: 0.6400\n",
            "F1 Score: 0.6589\n",
            "Accuracy: 0.6301\n",
            "Precision: 0.6092\n",
            "Recall: 0.7175\n",
            "AUC: 0.6733\n",
            "------------------------------\n",
            "Epoch 10/13\n",
            "Loss: 0.6376\n",
            "F1 Score: 0.6593\n",
            "Accuracy: 0.6272\n",
            "Precision: 0.6050\n",
            "Recall: 0.7244\n",
            "AUC: 0.6772\n",
            "------------------------------\n",
            "Epoch 11/13\n",
            "Loss: 0.6262\n",
            "F1 Score: 0.6761\n",
            "Accuracy: 0.6521\n",
            "Precision: 0.6303\n",
            "Recall: 0.7291\n",
            "AUC: 0.6987\n",
            "------------------------------\n",
            "Epoch 12/13\n",
            "Loss: 0.6130\n",
            "F1 Score: 0.6925\n",
            "Accuracy: 0.6670\n",
            "Precision: 0.6411\n",
            "Recall: 0.7528\n",
            "AUC: 0.7214\n",
            "------------------------------\n",
            "Epoch 13/13\n",
            "Loss: 0.5926\n",
            "F1 Score: 0.7081\n",
            "Accuracy: 0.6885\n",
            "Precision: 0.6639\n",
            "Recall: 0.7585\n",
            "AUC: 0.7451\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('gdrive/My Drive/anlp_project/bilstm_attention_model.pth')"
      ],
      "metadata": {
        "id": "wn-RdrlGAynO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4l8JPh9BM4X",
        "outputId": "f4c5bc77-a690-42fb-d460-4dc872b22d27"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "S75vCrBCBm98"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "test_loss = 0.0\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "test_probabilities = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for inputs, labels in test_dataloader:  # Assuming you have a DataLoader named test_dataloader\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs).squeeze(1)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predictions = (probabilities > 0.5).float()\n",
        "        test_predictions.extend(predictions.cpu().numpy())\n",
        "        test_probabilities.extend(probabilities.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_dataloader)\n",
        "test_f1 = f1_score(test_labels, test_predictions)\n",
        "test_acc = accuracy_score(test_labels, test_predictions)\n",
        "test_precision = precision_score(test_labels, test_predictions)\n",
        "test_recall = recall_score(test_labels, test_predictions)\n",
        "test_auc = roc_auc_score(test_labels, test_probabilities)\n",
        "\n",
        "print(\"Test Evaluation:\")\n",
        "print(f\"Loss: {test_loss:.4f}\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "print(f\"AUC: {test_auc:.4f}\")\n",
        "print('-' * 30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbnRAWWo5dNt",
        "outputId": "813c042d-4d9f-4540-bfa7-9ee47e05030e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Evaluation:\n",
            "Loss: 0.6499\n",
            "F1 Score: 0.5510\n",
            "Accuracy: 0.6071\n",
            "Precision: 0.5040\n",
            "Recall: 0.6077\n",
            "AUC: 0.6638\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "best_threshold = 0.5  # Default threshold\n",
        "best_f1 = 0.0\n",
        "\n",
        "# Iterate over potential thresholds\n",
        "thresholds = np.linspace(0, 1, 100)\n",
        "for threshold in thresholds:\n",
        "    predictions = (np.array(test_probabilities) > threshold).astype(int)\n",
        "    f1 = f1_score(test_labels, predictions)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = threshold\n",
        "\n",
        "test_predictions = (np.array(test_probabilities) > best_threshold).astype(int)\n",
        "test_acc = accuracy_score(test_labels, test_predictions)\n",
        "test_precision = precision_score(test_labels, test_predictions)\n",
        "test_recall = recall_score(test_labels, test_predictions)\n",
        "test_auc = roc_auc_score(test_labels, test_probabilities)\n",
        "\n",
        "print(\"Test Evaluation:\")\n",
        "print(f\"Loss: {test_loss:.4f}\")\n",
        "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
        "print(f\"F1 Score: {best_f1:.4f}\")\n",
        "print(f\"Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "print(f\"AUC: {test_auc:.4f}\")\n",
        "print('-' * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUGnbklHCQuO",
        "outputId": "cc70ea9f-5ce3-48fe-ab6b-118ffd9de6a0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Evaluation:\n",
            "Loss: 0.6499\n",
            "Best Threshold: 0.1818\n",
            "F1 Score: 0.6092\n",
            "Accuracy: 0.5255\n",
            "Precision: 0.4524\n",
            "Recall: 0.9325\n",
            "AUC: 0.6638\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), 'gdrive/My Drive/anlp_project/bilstm_attention_model_fasttext.pth')"
      ],
      "metadata": {
        "id": "2tKifzqBTLQA"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K Fold training"
      ],
      "metadata": {
        "id": "VdIAwDvQUeuq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "# Constants\n",
        "num_epochs = 20\n",
        "k_folds = 5\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "#kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "kfold = StratifiedKFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "results = {}  # To store results of each fold\n",
        "\n",
        "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_df, train_df['Label'])):  # Note that we pass train_df['Label'] as the second argument\n",
        "\n",
        "    # Sample elements randomly from a given list of ids, no replacement.\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_subsampler)\n",
        "    val_loader = DataLoader(train_dataset, batch_size=32, sampler=val_subsampler)\n",
        "\n",
        "    print(f'FOLD {fold + 1}')\n",
        "    print('-' * 40)\n",
        "\n",
        "    # Assuming `model` and optimizer are fresh and initialized\n",
        "    # Consider re-initializing model and optimizer at the start of each fold\n",
        "    model = BiLSTM_Attention(input_dim=400, hidden_dim=256, num_layers=2, output_dim=1)\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)  # learning rate of 0.001\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_probabilities = []\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs).squeeze(1)\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > 0.5).float()\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_probabilities.extend(probabilities.cpu().detach().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Logging for each epoch\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        f1 = f1_score(all_labels, all_predictions)\n",
        "        acc = accuracy_score(all_labels, all_predictions)\n",
        "        precision = precision_score(all_labels, all_predictions)\n",
        "        recall = recall_score(all_labels, all_predictions)\n",
        "        auc = roc_auc_score(all_labels, all_probabilities)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Train F1: {f1:.4f}, Train Acc: {acc:.4f}, Train AUC: {auc:.4f} \")\n",
        "\n",
        "        # Validation loss\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_predictions = []\n",
        "        val_labels = []\n",
        "        val_probabilities = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs).squeeze(1)\n",
        "                loss = criterion(outputs, labels.float())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                probabilities = torch.sigmoid(outputs)\n",
        "                predictions = (probabilities > 0.5).float()\n",
        "                val_predictions.extend(predictions.cpu().numpy())\n",
        "                val_probabilities.extend(probabilities.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_f1 = f1_score(val_labels, val_predictions)\n",
        "        val_acc = accuracy_score(val_labels, val_predictions)\n",
        "        val_auc = roc_auc_score(val_labels, val_probabilities)\n",
        "        results[fold] = val_acc  # Store the accuracy for this fold\n",
        "\n",
        "        print(f\"Validation Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "        print('-' * 30)\n",
        "\n",
        "# Print final results\n",
        "print(f\"K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS\")\n",
        "print('-' * 40)\n",
        "print(f\"Fold | Accuracy\")\n",
        "print('-' * 40)\n",
        "for key, value in results.items():\n",
        "    print(f\"{key + 1} | {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqM134vGs5W1",
        "outputId": "2c082458-8d32-41b3-a014-00a595016f4e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 1\n",
            "----------------------------------------\n",
            "Epoch 1/20 - Train Loss: 0.6947, Train F1: 0.4865, Train Acc: 0.4884, Train AUC: 0.4851 \n",
            "Validation Loss: 0.6932, Val F1: 0.6655, Val Acc: 0.4987, Val AUC: 0.5411\n",
            "------------------------------\n",
            "Epoch 2/20 - Train Loss: 0.6931, Train F1: 0.5071, Train Acc: 0.5123, Train AUC: 0.5153 \n",
            "Validation Loss: 0.6914, Val F1: 0.6750, Val Acc: 0.5223, Val AUC: 0.6175\n",
            "------------------------------\n",
            "Epoch 3/20 - Train Loss: 0.6906, Train F1: 0.5748, Train Acc: 0.5179, Train AUC: 0.5220 \n",
            "Validation Loss: 0.6902, Val F1: 0.6797, Val Acc: 0.5484, Val AUC: 0.5651\n",
            "------------------------------\n",
            "Epoch 4/20 - Train Loss: 0.6868, Train F1: 0.6088, Train Acc: 0.5319, Train AUC: 0.5339 \n",
            "Validation Loss: 0.7094, Val F1: 0.5529, Val Acc: 0.5131, Val AUC: 0.5636\n",
            "------------------------------\n",
            "Epoch 5/20 - Train Loss: 0.6843, Train F1: 0.6041, Train Acc: 0.5300, Train AUC: 0.5289 \n",
            "Validation Loss: 0.6769, Val F1: 0.5486, Val Acc: 0.5262, Val AUC: 0.5768\n",
            "------------------------------\n",
            "Epoch 6/20 - Train Loss: 0.6766, Train F1: 0.6373, Train Acc: 0.5604, Train AUC: 0.5559 \n",
            "Validation Loss: 0.6691, Val F1: 0.6591, Val Acc: 0.5668, Val AUC: 0.6000\n",
            "------------------------------\n",
            "Epoch 7/20 - Train Loss: 0.6708, Train F1: 0.6539, Train Acc: 0.5729, Train AUC: 0.5851 \n",
            "Validation Loss: 0.6721, Val F1: 0.6228, Val Acc: 0.5798, Val AUC: 0.5812\n",
            "------------------------------\n",
            "Epoch 8/20 - Train Loss: 0.6696, Train F1: 0.6511, Train Acc: 0.5752, Train AUC: 0.5927 \n",
            "Validation Loss: 0.6798, Val F1: 0.6740, Val Acc: 0.5707, Val AUC: 0.5785\n",
            "------------------------------\n",
            "Epoch 9/20 - Train Loss: 0.6664, Train F1: 0.6326, Train Acc: 0.5762, Train AUC: 0.6150 \n",
            "Validation Loss: 0.6772, Val F1: 0.6157, Val Acc: 0.5589, Val AUC: 0.5669\n",
            "------------------------------\n",
            "Epoch 10/20 - Train Loss: 0.6660, Train F1: 0.6362, Train Acc: 0.5850, Train AUC: 0.6168 \n",
            "Validation Loss: 0.6772, Val F1: 0.5868, Val Acc: 0.5576, Val AUC: 0.5840\n",
            "------------------------------\n",
            "Epoch 11/20 - Train Loss: 0.6610, Train F1: 0.6448, Train Acc: 0.5879, Train AUC: 0.6175 \n",
            "Validation Loss: 0.6748, Val F1: 0.5834, Val Acc: 0.5720, Val AUC: 0.5952\n",
            "------------------------------\n",
            "Epoch 12/20 - Train Loss: 0.6501, Train F1: 0.6640, Train Acc: 0.6161, Train AUC: 0.6508 \n",
            "Validation Loss: 0.6778, Val F1: 0.6247, Val Acc: 0.5942, Val AUC: 0.6196\n",
            "------------------------------\n",
            "Epoch 13/20 - Train Loss: 0.6443, Train F1: 0.6554, Train Acc: 0.6207, Train AUC: 0.6574 \n",
            "Validation Loss: 0.6689, Val F1: 0.6728, Val Acc: 0.5838, Val AUC: 0.6286\n",
            "------------------------------\n",
            "Epoch 14/20 - Train Loss: 0.6332, Train F1: 0.6798, Train Acc: 0.6423, Train AUC: 0.6782 \n",
            "Validation Loss: 0.6649, Val F1: 0.6681, Val Acc: 0.6021, Val AUC: 0.6405\n",
            "------------------------------\n",
            "Epoch 15/20 - Train Loss: 0.6150, Train F1: 0.6937, Train Acc: 0.6587, Train AUC: 0.7117 \n",
            "Validation Loss: 0.6814, Val F1: 0.5722, Val Acc: 0.5969, Val AUC: 0.6392\n",
            "------------------------------\n",
            "Epoch 16/20 - Train Loss: 0.6039, Train F1: 0.6945, Train Acc: 0.6711, Train AUC: 0.7280 \n",
            "Validation Loss: 0.6948, Val F1: 0.5827, Val Acc: 0.5969, Val AUC: 0.6475\n",
            "------------------------------\n",
            "Epoch 17/20 - Train Loss: 0.5899, Train F1: 0.7021, Train Acc: 0.6820, Train AUC: 0.7430 \n",
            "Validation Loss: 0.6685, Val F1: 0.6292, Val Acc: 0.6204, Val AUC: 0.6612\n",
            "------------------------------\n",
            "Epoch 18/20 - Train Loss: 0.5690, Train F1: 0.7239, Train Acc: 0.7062, Train AUC: 0.7721 \n",
            "Validation Loss: 0.6855, Val F1: 0.6606, Val Acc: 0.6073, Val AUC: 0.6470\n",
            "------------------------------\n",
            "Epoch 19/20 - Train Loss: 0.5346, Train F1: 0.7461, Train Acc: 0.7321, Train AUC: 0.8030 \n",
            "Validation Loss: 0.6959, Val F1: 0.6268, Val Acc: 0.6243, Val AUC: 0.6625\n",
            "------------------------------\n",
            "Epoch 20/20 - Train Loss: 0.5128, Train F1: 0.7567, Train Acc: 0.7429, Train AUC: 0.8228 \n",
            "Validation Loss: 0.6944, Val F1: 0.6538, Val Acc: 0.6230, Val AUC: 0.6686\n",
            "------------------------------\n",
            "FOLD 2\n",
            "----------------------------------------\n",
            "Epoch 1/20 - Train Loss: 0.6944, Train F1: 0.4114, Train Acc: 0.5061, Train AUC: 0.5107 \n",
            "Validation Loss: 0.6943, Val F1: 0.0000, Val Acc: 0.5026, Val AUC: 0.5149\n",
            "------------------------------\n",
            "Epoch 2/20 - Train Loss: 0.6917, Train F1: 0.5479, Train Acc: 0.5087, Train AUC: 0.5145 \n",
            "Validation Loss: 0.6849, Val F1: 0.5656, Val Acc: 0.5236, Val AUC: 0.5484\n",
            "------------------------------\n",
            "Epoch 3/20 - Train Loss: 0.6926, Train F1: 0.5713, Train Acc: 0.5398, Train AUC: 0.5546 \n",
            "Validation Loss: 0.6804, Val F1: 0.4959, Val Acc: 0.5131, Val AUC: 0.5592\n",
            "------------------------------\n",
            "Epoch 4/20 - Train Loss: 0.6842, Train F1: 0.5614, Train Acc: 0.5368, Train AUC: 0.5548 \n",
            "Validation Loss: 0.6768, Val F1: 0.5194, Val Acc: 0.5301, Val AUC: 0.5732\n",
            "------------------------------\n",
            "Epoch 5/20 - Train Loss: 0.6767, Train F1: 0.5879, Train Acc: 0.5385, Train AUC: 0.5741 \n",
            "Validation Loss: 0.6690, Val F1: 0.6604, Val Acc: 0.5746, Val AUC: 0.5937\n",
            "------------------------------\n",
            "Epoch 6/20 - Train Loss: 0.6635, Train F1: 0.6364, Train Acc: 0.5929, Train AUC: 0.6237 \n",
            "Validation Loss: 0.6705, Val F1: 0.6572, Val Acc: 0.5877, Val AUC: 0.6116\n",
            "------------------------------\n",
            "Epoch 7/20 - Train Loss: 0.6567, Train F1: 0.6332, Train Acc: 0.6050, Train AUC: 0.6379 \n",
            "Validation Loss: 0.6967, Val F1: 0.6797, Val Acc: 0.5484, Val AUC: 0.6041\n",
            "------------------------------\n",
            "Epoch 8/20 - Train Loss: 0.6586, Train F1: 0.6522, Train Acc: 0.5948, Train AUC: 0.6302 \n",
            "Validation Loss: 0.6676, Val F1: 0.6503, Val Acc: 0.5903, Val AUC: 0.6164\n",
            "------------------------------\n",
            "Epoch 9/20 - Train Loss: 0.6418, Train F1: 0.6680, Train Acc: 0.6259, Train AUC: 0.6657 \n",
            "Validation Loss: 0.6704, Val F1: 0.6316, Val Acc: 0.5785, Val AUC: 0.6072\n",
            "------------------------------\n",
            "Epoch 10/20 - Train Loss: 0.6397, Train F1: 0.6587, Train Acc: 0.6273, Train AUC: 0.6735 \n",
            "Validation Loss: 0.6777, Val F1: 0.5860, Val Acc: 0.5969, Val AUC: 0.6169\n",
            "------------------------------\n",
            "Epoch 11/20 - Train Loss: 0.6211, Train F1: 0.6838, Train Acc: 0.6577, Train AUC: 0.7054 \n",
            "Validation Loss: 0.6765, Val F1: 0.6030, Val Acc: 0.5864, Val AUC: 0.6142\n",
            "------------------------------\n",
            "Epoch 12/20 - Train Loss: 0.6041, Train F1: 0.6999, Train Acc: 0.6767, Train AUC: 0.7280 \n",
            "Validation Loss: 0.6843, Val F1: 0.5722, Val Acc: 0.6047, Val AUC: 0.6365\n",
            "------------------------------\n",
            "Epoch 13/20 - Train Loss: 0.5808, Train F1: 0.7051, Train Acc: 0.6898, Train AUC: 0.7568 \n",
            "Validation Loss: 0.6920, Val F1: 0.6266, Val Acc: 0.6178, Val AUC: 0.6396\n",
            "------------------------------\n",
            "Epoch 14/20 - Train Loss: 0.5515, Train F1: 0.7293, Train Acc: 0.7141, Train AUC: 0.7857 \n",
            "Validation Loss: 0.7253, Val F1: 0.6076, Val Acc: 0.6060, Val AUC: 0.6338\n",
            "------------------------------\n",
            "Epoch 15/20 - Train Loss: 0.5329, Train F1: 0.7385, Train Acc: 0.7265, Train AUC: 0.8051 \n",
            "Validation Loss: 0.6815, Val F1: 0.6061, Val Acc: 0.6086, Val AUC: 0.6498\n",
            "------------------------------\n",
            "Epoch 16/20 - Train Loss: 0.5029, Train F1: 0.7544, Train Acc: 0.7462, Train AUC: 0.8272 \n",
            "Validation Loss: 0.7342, Val F1: 0.6511, Val Acc: 0.6086, Val AUC: 0.6390\n",
            "------------------------------\n",
            "Epoch 17/20 - Train Loss: 0.4730, Train F1: 0.7769, Train Acc: 0.7658, Train AUC: 0.8502 \n",
            "Validation Loss: 0.7249, Val F1: 0.6448, Val Acc: 0.6322, Val AUC: 0.6563\n",
            "------------------------------\n",
            "Epoch 18/20 - Train Loss: 0.4325, Train F1: 0.8090, Train Acc: 0.8035, Train AUC: 0.8788 \n",
            "Validation Loss: 0.8135, Val F1: 0.6521, Val Acc: 0.6243, Val AUC: 0.6367\n",
            "------------------------------\n",
            "Epoch 19/20 - Train Loss: 0.4042, Train F1: 0.8250, Train Acc: 0.8185, Train AUC: 0.8967 \n",
            "Validation Loss: 0.8505, Val F1: 0.6408, Val Acc: 0.6243, Val AUC: 0.6451\n",
            "------------------------------\n",
            "Epoch 20/20 - Train Loss: 0.3636, Train F1: 0.8431, Train Acc: 0.8411, Train AUC: 0.9163 \n",
            "Validation Loss: 0.8773, Val F1: 0.6507, Val Acc: 0.6191, Val AUC: 0.6397\n",
            "------------------------------\n",
            "FOLD 3\n",
            "----------------------------------------\n",
            "Epoch 1/20 - Train Loss: 0.6944, Train F1: 0.4112, Train Acc: 0.5049, Train AUC: 0.4998 \n",
            "Validation Loss: 0.6927, Val F1: 0.6595, Val Acc: 0.5007, Val AUC: 0.5511\n",
            "------------------------------\n",
            "Epoch 2/20 - Train Loss: 0.6911, Train F1: 0.3664, Train Acc: 0.5085, Train AUC: 0.5157 \n",
            "Validation Loss: 0.6959, Val F1: 0.6655, Val Acc: 0.4993, Val AUC: 0.5273\n",
            "------------------------------\n",
            "Epoch 3/20 - Train Loss: 0.6876, Train F1: 0.5673, Train Acc: 0.5065, Train AUC: 0.5225 \n",
            "Validation Loss: 0.6765, Val F1: 0.4973, Val Acc: 0.5203, Val AUC: 0.5595\n",
            "------------------------------\n",
            "Epoch 4/20 - Train Loss: 0.6955, Train F1: 0.4957, Train Acc: 0.5157, Train AUC: 0.5211 \n",
            "Validation Loss: 0.6827, Val F1: 0.4374, Val Acc: 0.5347, Val AUC: 0.5840\n",
            "------------------------------\n",
            "Epoch 5/20 - Train Loss: 0.6828, Train F1: 0.5883, Train Acc: 0.5491, Train AUC: 0.5651 \n",
            "Validation Loss: 0.6788, Val F1: 0.1974, Val Acc: 0.5203, Val AUC: 0.6125\n",
            "------------------------------\n",
            "Epoch 6/20 - Train Loss: 0.6758, Train F1: 0.6117, Train Acc: 0.5602, Train AUC: 0.5909 \n",
            "Validation Loss: 0.6737, Val F1: 0.6747, Val Acc: 0.5727, Val AUC: 0.6143\n",
            "------------------------------\n",
            "Epoch 7/20 - Train Loss: 0.6711, Train F1: 0.5837, Train Acc: 0.5697, Train AUC: 0.6008 \n",
            "Validation Loss: 0.6666, Val F1: 0.6159, Val Acc: 0.5701, Val AUC: 0.6078\n",
            "------------------------------\n",
            "Epoch 8/20 - Train Loss: 0.6641, Train F1: 0.6171, Train Acc: 0.5819, Train AUC: 0.6208 \n",
            "Validation Loss: 0.6696, Val F1: 0.6401, Val Acc: 0.5740, Val AUC: 0.6171\n",
            "------------------------------\n",
            "Epoch 9/20 - Train Loss: 0.6566, Train F1: 0.6199, Train Acc: 0.5940, Train AUC: 0.6369 \n",
            "Validation Loss: 0.6555, Val F1: 0.5794, Val Acc: 0.6042, Val AUC: 0.6495\n",
            "------------------------------\n",
            "Epoch 10/20 - Train Loss: 0.6544, Train F1: 0.6213, Train Acc: 0.5933, Train AUC: 0.6419 \n",
            "Validation Loss: 0.6514, Val F1: 0.6078, Val Acc: 0.6042, Val AUC: 0.6535\n",
            "------------------------------\n",
            "Epoch 11/20 - Train Loss: 0.6472, Train F1: 0.6407, Train Acc: 0.6097, Train AUC: 0.6546 \n",
            "Validation Loss: 0.6713, Val F1: 0.6674, Val Acc: 0.5845, Val AUC: 0.6411\n",
            "------------------------------\n",
            "Epoch 12/20 - Train Loss: 0.6434, Train F1: 0.6461, Train Acc: 0.6215, Train AUC: 0.6698 \n",
            "Validation Loss: 0.6592, Val F1: 0.6637, Val Acc: 0.5976, Val AUC: 0.6534\n",
            "------------------------------\n",
            "Epoch 13/20 - Train Loss: 0.6337, Train F1: 0.6474, Train Acc: 0.6280, Train AUC: 0.6881 \n",
            "Validation Loss: 0.6549, Val F1: 0.6520, Val Acc: 0.6278, Val AUC: 0.6748\n",
            "------------------------------\n",
            "Epoch 14/20 - Train Loss: 0.6265, Train F1: 0.6512, Train Acc: 0.6349, Train AUC: 0.6975 \n",
            "Validation Loss: 0.6450, Val F1: 0.6468, Val Acc: 0.6121, Val AUC: 0.6680\n",
            "------------------------------\n",
            "Epoch 15/20 - Train Loss: 0.6225, Train F1: 0.6556, Train Acc: 0.6454, Train AUC: 0.7027 \n",
            "Validation Loss: 0.6465, Val F1: 0.6659, Val Acc: 0.6186, Val AUC: 0.6674\n",
            "------------------------------\n",
            "Epoch 16/20 - Train Loss: 0.6094, Train F1: 0.6641, Train Acc: 0.6536, Train AUC: 0.7193 \n",
            "Validation Loss: 0.6340, Val F1: 0.6381, Val Acc: 0.6239, Val AUC: 0.6860\n",
            "------------------------------\n",
            "Epoch 17/20 - Train Loss: 0.5968, Train F1: 0.6756, Train Acc: 0.6660, Train AUC: 0.7361 \n",
            "Validation Loss: 0.6580, Val F1: 0.6343, Val Acc: 0.6252, Val AUC: 0.6761\n",
            "------------------------------\n",
            "Epoch 18/20 - Train Loss: 0.5788, Train F1: 0.6895, Train Acc: 0.6853, Train AUC: 0.7578 \n",
            "Validation Loss: 0.6570, Val F1: 0.6546, Val Acc: 0.6252, Val AUC: 0.6778\n",
            "------------------------------\n",
            "Epoch 19/20 - Train Loss: 0.5625, Train F1: 0.7077, Train Acc: 0.6948, Train AUC: 0.7718 \n",
            "Validation Loss: 0.6876, Val F1: 0.6310, Val Acc: 0.6291, Val AUC: 0.6757\n",
            "------------------------------\n",
            "Epoch 20/20 - Train Loss: 0.5317, Train F1: 0.7185, Train Acc: 0.7200, Train AUC: 0.8036 \n",
            "Validation Loss: 0.6640, Val F1: 0.6625, Val Acc: 0.6435, Val AUC: 0.6945\n",
            "------------------------------\n",
            "FOLD 4\n",
            "----------------------------------------\n",
            "Epoch 1/20 - Train Loss: 0.6966, Train F1: 0.5617, Train Acc: 0.4967, Train AUC: 0.4850 \n",
            "Validation Loss: 0.6932, Val F1: 0.5664, Val Acc: 0.4823, Val AUC: 0.4967\n",
            "------------------------------\n",
            "Epoch 2/20 - Train Loss: 0.6934, Train F1: 0.4122, Train Acc: 0.5013, Train AUC: 0.5053 \n",
            "Validation Loss: 0.6932, Val F1: 0.6649, Val Acc: 0.4980, Val AUC: 0.5648\n",
            "------------------------------\n",
            "Epoch 3/20 - Train Loss: 0.6900, Train F1: 0.6178, Train Acc: 0.5341, Train AUC: 0.5406 \n",
            "Validation Loss: 0.6869, Val F1: 0.5035, Val Acc: 0.5374, Val AUC: 0.5775\n",
            "------------------------------\n",
            "Epoch 4/20 - Train Loss: 0.6795, Train F1: 0.5840, Train Acc: 0.5560, Train AUC: 0.5780 \n",
            "Validation Loss: 0.6741, Val F1: 0.5845, Val Acc: 0.5714, Val AUC: 0.5825\n",
            "------------------------------\n",
            "Epoch 5/20 - Train Loss: 0.6681, Train F1: 0.6243, Train Acc: 0.5796, Train AUC: 0.6067 \n",
            "Validation Loss: 0.6703, Val F1: 0.6519, Val Acc: 0.5688, Val AUC: 0.6025\n",
            "------------------------------\n",
            "Epoch 6/20 - Train Loss: 0.6614, Train F1: 0.6486, Train Acc: 0.6012, Train AUC: 0.6278 \n",
            "Validation Loss: 0.6630, Val F1: 0.6533, Val Acc: 0.5911, Val AUC: 0.6175\n",
            "------------------------------\n",
            "Epoch 7/20 - Train Loss: 0.6574, Train F1: 0.6492, Train Acc: 0.6090, Train AUC: 0.6453 \n",
            "Validation Loss: 0.6616, Val F1: 0.6127, Val Acc: 0.5858, Val AUC: 0.6336\n",
            "------------------------------\n",
            "Epoch 8/20 - Train Loss: 0.6581, Train F1: 0.6371, Train Acc: 0.6031, Train AUC: 0.6385 \n",
            "Validation Loss: 0.6608, Val F1: 0.5575, Val Acc: 0.5963, Val AUC: 0.6344\n",
            "------------------------------\n",
            "Epoch 9/20 - Train Loss: 0.6528, Train F1: 0.6371, Train Acc: 0.6172, Train AUC: 0.6604 \n",
            "Validation Loss: 0.6580, Val F1: 0.6441, Val Acc: 0.6003, Val AUC: 0.6414\n",
            "------------------------------\n",
            "Epoch 10/20 - Train Loss: 0.6306, Train F1: 0.6656, Train Acc: 0.6500, Train AUC: 0.6973 \n",
            "Validation Loss: 0.6548, Val F1: 0.6540, Val Acc: 0.6173, Val AUC: 0.6567\n",
            "------------------------------\n",
            "Epoch 11/20 - Train Loss: 0.6309, Train F1: 0.6451, Train Acc: 0.6379, Train AUC: 0.6948 \n",
            "Validation Loss: 0.6549, Val F1: 0.6211, Val Acc: 0.6003, Val AUC: 0.6522\n",
            "------------------------------\n",
            "Epoch 12/20 - Train Loss: 0.6137, Train F1: 0.6806, Train Acc: 0.6637, Train AUC: 0.7197 \n",
            "Validation Loss: 0.6602, Val F1: 0.6538, Val Acc: 0.5990, Val AUC: 0.6525\n",
            "------------------------------\n",
            "Epoch 13/20 - Train Loss: 0.5914, Train F1: 0.6943, Train Acc: 0.6866, Train AUC: 0.7501 \n",
            "Validation Loss: 0.6565, Val F1: 0.6509, Val Acc: 0.6134, Val AUC: 0.6677\n",
            "------------------------------\n",
            "Epoch 14/20 - Train Loss: 0.5802, Train F1: 0.7092, Train Acc: 0.6968, Train AUC: 0.7618 \n",
            "Validation Loss: 0.6426, Val F1: 0.6706, Val Acc: 0.6343, Val AUC: 0.6858\n",
            "------------------------------\n",
            "Epoch 15/20 - Train Loss: 0.5560, Train F1: 0.7256, Train Acc: 0.7164, Train AUC: 0.7860 \n",
            "Validation Loss: 0.6724, Val F1: 0.6780, Val Acc: 0.6265, Val AUC: 0.6731\n",
            "------------------------------\n",
            "Epoch 16/20 - Train Loss: 0.5359, Train F1: 0.7370, Train Acc: 0.7299, Train AUC: 0.8066 \n",
            "Validation Loss: 0.6588, Val F1: 0.6264, Val Acc: 0.6186, Val AUC: 0.6776\n",
            "------------------------------\n",
            "Epoch 17/20 - Train Loss: 0.5205, Train F1: 0.7495, Train Acc: 0.7420, Train AUC: 0.8174 \n",
            "Validation Loss: 0.6821, Val F1: 0.6085, Val Acc: 0.6003, Val AUC: 0.6683\n",
            "------------------------------\n",
            "Epoch 18/20 - Train Loss: 0.4946, Train F1: 0.7629, Train Acc: 0.7570, Train AUC: 0.8384 \n",
            "Validation Loss: 0.7456, Val F1: 0.5739, Val Acc: 0.6107, Val AUC: 0.6656\n",
            "------------------------------\n",
            "Epoch 19/20 - Train Loss: 0.4479, Train F1: 0.7941, Train Acc: 0.7937, Train AUC: 0.8715 \n",
            "Validation Loss: 0.7622, Val F1: 0.6142, Val Acc: 0.6016, Val AUC: 0.6689\n",
            "------------------------------\n",
            "Epoch 20/20 - Train Loss: 0.3985, Train F1: 0.8248, Train Acc: 0.8232, Train AUC: 0.9011 \n",
            "Validation Loss: 0.8094, Val F1: 0.6175, Val Acc: 0.5990, Val AUC: 0.6577\n",
            "------------------------------\n",
            "FOLD 5\n",
            "----------------------------------------\n",
            "Epoch 1/20 - Train Loss: 0.6955, Train F1: 0.5343, Train Acc: 0.5115, Train AUC: 0.5024 \n",
            "Validation Loss: 0.6929, Val F1: 0.0000, Val Acc: 0.5007, Val AUC: 0.5654\n",
            "------------------------------\n",
            "Epoch 2/20 - Train Loss: 0.6925, Train F1: 0.3681, Train Acc: 0.4987, Train AUC: 0.5117 \n",
            "Validation Loss: 0.6880, Val F1: 0.1070, Val Acc: 0.4967, Val AUC: 0.5268\n",
            "------------------------------\n",
            "Epoch 3/20 - Train Loss: 0.6862, Train F1: 0.5916, Train Acc: 0.5311, Train AUC: 0.5360 \n",
            "Validation Loss: 0.6855, Val F1: 0.5099, Val Acc: 0.4784, Val AUC: 0.5235\n",
            "------------------------------\n",
            "Epoch 4/20 - Train Loss: 0.6797, Train F1: 0.6164, Train Acc: 0.5504, Train AUC: 0.5653 \n",
            "Validation Loss: 0.7008, Val F1: 0.0000, Val Acc: 0.5020, Val AUC: 0.5471\n",
            "------------------------------\n",
            "Epoch 5/20 - Train Loss: 0.6763, Train F1: 0.6451, Train Acc: 0.5720, Train AUC: 0.5807 \n",
            "Validation Loss: 0.6759, Val F1: 0.6234, Val Acc: 0.5819, Val AUC: 0.5778\n",
            "------------------------------\n",
            "Epoch 6/20 - Train Loss: 0.6664, Train F1: 0.6538, Train Acc: 0.5842, Train AUC: 0.6021 \n",
            "Validation Loss: 0.6755, Val F1: 0.6525, Val Acc: 0.5714, Val AUC: 0.5910\n",
            "------------------------------\n",
            "Epoch 7/20 - Train Loss: 0.6604, Train F1: 0.6621, Train Acc: 0.5969, Train AUC: 0.6210 \n",
            "Validation Loss: 0.6731, Val F1: 0.6514, Val Acc: 0.5819, Val AUC: 0.6012\n",
            "------------------------------\n",
            "Epoch 8/20 - Train Loss: 0.6555, Train F1: 0.6654, Train Acc: 0.6087, Train AUC: 0.6319 \n",
            "Validation Loss: 0.6705, Val F1: 0.6311, Val Acc: 0.5740, Val AUC: 0.6099\n",
            "------------------------------\n",
            "Epoch 9/20 - Train Loss: 0.6484, Train F1: 0.6768, Train Acc: 0.6185, Train AUC: 0.6450 \n",
            "Validation Loss: 0.6690, Val F1: 0.6344, Val Acc: 0.5740, Val AUC: 0.6175\n",
            "------------------------------\n",
            "Epoch 10/20 - Train Loss: 0.6469, Train F1: 0.6588, Train Acc: 0.6117, Train AUC: 0.6527 \n",
            "Validation Loss: 0.6617, Val F1: 0.6458, Val Acc: 0.6003, Val AUC: 0.6379\n",
            "------------------------------\n",
            "Epoch 11/20 - Train Loss: 0.6321, Train F1: 0.6731, Train Acc: 0.6313, Train AUC: 0.6807 \n",
            "Validation Loss: 0.6614, Val F1: 0.6710, Val Acc: 0.6003, Val AUC: 0.6492\n",
            "------------------------------\n",
            "Epoch 12/20 - Train Loss: 0.6248, Train F1: 0.6826, Train Acc: 0.6470, Train AUC: 0.6979 \n",
            "Validation Loss: 0.6456, Val F1: 0.6674, Val Acc: 0.6225, Val AUC: 0.6760\n",
            "------------------------------\n",
            "Epoch 13/20 - Train Loss: 0.6248, Train F1: 0.6597, Train Acc: 0.6362, Train AUC: 0.6982 \n",
            "Validation Loss: 0.6616, Val F1: 0.6329, Val Acc: 0.6199, Val AUC: 0.6758\n",
            "------------------------------\n",
            "Epoch 14/20 - Train Loss: 0.5989, Train F1: 0.6865, Train Acc: 0.6693, Train AUC: 0.7330 \n",
            "Validation Loss: 0.6601, Val F1: 0.6487, Val Acc: 0.6068, Val AUC: 0.6718\n",
            "------------------------------\n",
            "Epoch 15/20 - Train Loss: 0.5802, Train F1: 0.7092, Train Acc: 0.6880, Train AUC: 0.7545 \n",
            "Validation Loss: 0.6472, Val F1: 0.6674, Val Acc: 0.6265, Val AUC: 0.6835\n",
            "------------------------------\n",
            "Epoch 16/20 - Train Loss: 0.5586, Train F1: 0.7263, Train Acc: 0.7092, Train AUC: 0.7810 \n",
            "Validation Loss: 0.6692, Val F1: 0.5901, Val Acc: 0.6212, Val AUC: 0.6869\n",
            "------------------------------\n",
            "Epoch 17/20 - Train Loss: 0.5425, Train F1: 0.7317, Train Acc: 0.7207, Train AUC: 0.7958 \n",
            "Validation Loss: 0.6690, Val F1: 0.6831, Val Acc: 0.6134, Val AUC: 0.6848\n",
            "------------------------------\n",
            "Epoch 18/20 - Train Loss: 0.5175, Train F1: 0.7492, Train Acc: 0.7338, Train AUC: 0.8179 \n",
            "Validation Loss: 0.6833, Val F1: 0.6232, Val Acc: 0.6212, Val AUC: 0.6797\n",
            "------------------------------\n",
            "Epoch 19/20 - Train Loss: 0.4827, Train F1: 0.7704, Train Acc: 0.7633, Train AUC: 0.8451 \n",
            "Validation Loss: 0.6953, Val F1: 0.6580, Val Acc: 0.6199, Val AUC: 0.6839\n",
            "------------------------------\n",
            "Epoch 20/20 - Train Loss: 0.4507, Train F1: 0.7954, Train Acc: 0.7885, Train AUC: 0.8683 \n",
            "Validation Loss: 0.7468, Val F1: 0.6509, Val Acc: 0.6121, Val AUC: 0.6694\n",
            "------------------------------\n",
            "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
            "----------------------------------------\n",
            "Fold | Accuracy\n",
            "----------------------------------------\n",
            "1 | 0.6230\n",
            "2 | 0.6191\n",
            "3 | 0.6435\n",
            "4 | 0.5990\n",
            "5 | 0.6121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jq_c9Z_9vCHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}